{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tiled Cholesky Factorization\n",
    "\n",
    "\n",
    "We illustrate here the use of `DataFlowTasks` to parallelize a tiled Cholesky\n",
    "factorization. The implementation shown here is delibarately made as simple\n",
    "and self-contained as possible; a more complex and more efficient\n",
    "implementation can be found in the\n",
    "[TiledFactorization](https://github.com/maltezfaria/TiledFactorization)\n",
    "package."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Cholesky factorization algorithm takes a symmetric positive definite\n",
    "matrix $A$ and finds a lower triangular matrix $L$ such that $A = LLᵀ$. The\n",
    "tiled version of this algorithm decomposes the matrix $A$ into tiles (of even\n",
    "sizes, in this simplified version). At each step of the algorithm, we do a\n",
    "Cholesky factorization on the diagonal tile, use a triangular solve to update\n",
    "all of the tiles at the right of the diagonal tile, and finally update all the\n",
    "tiles of the submatrix with a schur complement.\n",
    "\n",
    "If we have a matrix $A$ decomposed in $n \\times n$ tiles, then the algorithm will\n",
    "have $n$ steps. The $i$-th step (with $i \\in [1:n]$) performs:\n",
    "\n",
    "-  $1$ cholesky factorization of the $(i,i)$ tile,\n",
    "-  $(i-1)$ triangular solves (one for each tile in the $i$-th row of the upper triangular matrix),\n",
    "-  $i(i-1)/2$ matrix multiplications to update the submatrix.\n",
    "\n",
    "These are the basic operations on tiles, which we are going to spawn in\n",
    "separate tasks in the parallel implementation. Accounting for all iterations,\n",
    "this makes a total of $\\mathcal{O}(n^3)$ such tasks, decomposed as:\n",
    "\n",
    "-  $\\mathcal{O}(n)$ cholesky factorizations,\n",
    "-  $\\mathcal{O}(n^2)$ triangular solves,\n",
    "-  $\\mathcal{O}(n^3)$ matrix multiplications."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following image illustrates the 2nd step of the algorithm:\n",
    "\n",
    "![](Cholesky_2ndStep.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequential implementation\n",
    "\n",
    "A sequential tiled factorization algorithm can be implemented as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "cholesky_tiled! (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "tilerange(ti, ts) = (ti-1)*ts+1:ti*ts\n",
    "\n",
    "function cholesky_tiled!(A, ts)\n",
    "    m = size(A, 1); @assert m==size(A, 2)\n",
    "    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n",
    "    n = m÷ts  # number of tiles in each dimension\n",
    "\n",
    "    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n",
    "\n",
    "    for i in 1:n\n",
    "        # Diagonal cholesky serial factorization\n",
    "        cholesky!(T[i,i])\n",
    "\n",
    "        # Left tiles update\n",
    "        U = UpperTriangular(T[i,i])\n",
    "        for j in i+1:n\n",
    "            ldiv!(U', T[i,j])\n",
    "        end\n",
    "\n",
    "        # Submatrix update\n",
    "        for j in i+1:n\n",
    "            for k in j:n\n",
    "                mul!(T[j,k], T[i,j]', T[i,k], -1, 1)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Construct the factorized object\n",
    "    return Cholesky(A, 'U', zero(LinearAlgebra.BlasInt))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us build a small test case to check the correctness of the\n",
    "factorization. Here we divide a matrix of size 4096×4096 in 8×8 tiles of size\n",
    "512×512:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n  = 4096\n",
    "ts = 512\n",
    "A = rand(n, n)\n",
    "A = (A + adjoint(A))/2\n",
    "A = A + n*I;"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "and the results seem to be correct:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err = 2.4282273077008298e-17\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "F = cholesky_tiled!(copy(A), ts)\n",
    "\n",
    "# Check results\n",
    "err = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n",
    "@show err\n",
    "@assert err < eps(Float64)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallel implementation\n",
    "\n",
    "In order to parallelize the code with `DataFlowTasks.jl`, function calls\n",
    "acting on tiles are wrapped within `@dspawn`, along with annotations\n",
    "describing data access modes. We also give meaningful labels to the tasks,\n",
    "which will help debug and profile the code."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "cholesky_dft! (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "using DataFlowTasks\n",
    "\n",
    "function cholesky_dft!(A, ts)\n",
    "    m = size(A, 1); @assert m==size(A, 2)\n",
    "    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n",
    "    n = m÷ts  # number of tiles in each dimension\n",
    "\n",
    "    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n",
    "\n",
    "    for i in 1:n\n",
    "        # Diagonal cholesky serial factorization\n",
    "        @dspawn cholesky!(@RW(T[i,i])) label=\"chol ($i,$i)\"\n",
    "\n",
    "        # Left tiles update\n",
    "        U = UpperTriangular(T[i,i])\n",
    "        for j in i+1:n\n",
    "            @dspawn ldiv!(@R(U)', @RW(T[i,j])) label=\"ldiv ($i,$j)\"\n",
    "        end\n",
    "\n",
    "        # Submatrix update\n",
    "        for j in i+1:n\n",
    "            for k in j:n\n",
    "                @dspawn mul!(@RW(T[j,k]), @R(T[i,j])', @R(T[i,k]), -1, 1) label=\"schur ($j,$k)\"\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Construct the factorized object\n",
    "    r = @dspawn Cholesky(@R(A), 'U', zero(LinearAlgebra.BlasInt)) label=\"result\"\n",
    "    return fetch(r)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, let us check the correctness of the result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err = 2.4282273077008298e-17\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "F = cholesky_dft!(copy(A), ts)\n",
    "\n",
    "# Check results\n",
    "err = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n",
    "@show err\n",
    "@assert err < eps(Float64)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debugging and Profiling\n",
    "\n",
    "Let us now check what happens during a parallel run of our cholesky\n",
    "factorization. Thanks to the test above, the code is now compiled. Let's re-run it and collect\n",
    "meaningful profiling information:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Clean profiling environment\n",
    "GC.gc()\n",
    "\n",
    "# Real workload to be analysed\n",
    "Ac = copy(A)\n",
    "logger = DataFlowTasks.@log cholesky_dft!(Ac, ts);"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of tasks being $\\mathcal{O}(n^3)$, we can see how quickly the DAG\n",
    "complexity increases (even though the test case only has 8×8 tiles here):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Loading DataFlowTasks dag plot utilities\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using GraphViz\n",
    "dag = DataFlowTasks.dagplot(logger)\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "The critical path, highlighted in red, includes all cholesky factorizations of\n",
    "diagonal tiles, as well as the required tasks in between them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parallel trace plot gives us more details about the\n",
    "performance limiting factors:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Loading DataFlowTasks general plot utilities\n",
      "[ Info: Computing    : 0.5513323359999999\n",
      "[ Info: Inserting    : 0.0007004990000000006\n",
      "[ Info: Other        : 0.00035190000003215954\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using CairoMakie # or GLMakie in order to have more interactivity\n",
    "trace = DataFlowTasks.plot(logger;categories=[\"chol\", \"ldiv\", \"schur\"])\n",
    "nothing #hide"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The overhead incurred by `DataFlowTasks` seems relatively small here: the time\n",
    "taken inserting tasks is barely measurable, and the scheduling did not lead to\n",
    "threads waiting idly for too long. This is confirmed by the \"Time Bounds\"\n",
    "plot, showing a measured wall clock time not too much longer than the lower\n",
    "bound obtained when suppressing idle time.\n",
    "\n",
    "The \"Times per Category\" plot seems to indicate that the matrix\n",
    "multiplications performed in the \"Schur\" tasks account for the majority of the\n",
    "computing time. Trying to optimize these would be a priority to increase the\n",
    "sequential performance of the factorization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The performance of this example can be improved by using better\n",
    "implementations for the sequential building blocks operating on tiles:\n",
    "\n",
    "- `LoopVectorization.jl` can improve the performance of the sequential\n",
    "  cholesky factorization of diagonal blocks as well as the `schur_complement`\n",
    "- `TriangularSolve.jl` provides a high-performance `ldiv!` implementation\n",
    "\n",
    "This approach is pursued in\n",
    "[`TiledFactorization.jl`](https://github.com/maltezfaria/TiledFactorization),\n",
    "where all the above mentioned building blocks are combined with the\n",
    "parallelization strategy presented here to create a *pure Julia*\n",
    "implementation of the matrix factorizations. The performances of this\n",
    "implementation is assessed in the following plot, by comparison to MKL on a\n",
    "the case of a 5000x5000 matrix decomposed in tiles of size 256x256.\n",
    "\n",
    "![](cholesky_scaling.png)\n",
    "\n",
    "The figure above was generated by running [this\n",
    "script](https://github.com/maltezfaria/TiledFactorization/blob/daafed7b6981853b6c71e7441fd9b212582836db/benchmarks/cholesky_scaling.sh)\n",
    "on a machine with 2x10 Intel Xeon Silver 4114 cores (2.20GHz) with the following topology:\n",
    "\n",
    "![](lfaria-precision-7920-tower-lstopo.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "kernelspec": {
   "name": "julia-1.8",
   "display_name": "Julia 1.8.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
