var documenterSearchIndex = {"docs":
[{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"EditURL = \"https://github.com/maltezfaria/DataFlowTasks.jl/blob/main/docs/src/examples/cholesky/cholesky.jl\"","category":"page"},{"location":"examples/cholesky/cholesky/#tiledcholesky-section","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: ipynb) (Image: nbviewer)","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"We illustrate here the use of DataFlowTasks to parallelize a tiled Cholesky factorization. The implementation shown here is delibarately made as simple and self-contained as possible; a more complex and more efficient implementation can be found in the TiledFactorization package.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The Cholesky factorization algorithm takes a symmetric positive definite matrix A and finds a lower triangular matrix L such that A = LLᵀ. The tiled version of this algorithm decomposes the matrix A into tiles (of even sizes, in this simplified version). At each step of the algorithm, we do a Cholesky factorization on the diagonal tile, use a triangular solve to update all of the tiles at the right of the diagonal tile, and finally update all the tiles of the submatrix with a schur complement.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"If we have a matrix A decomposed in n times n tiles, then the algorithm will have n steps. The i-th step (with i in 1n) performs:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":" 1 cholesky factorization of the (ii) tile,\n (i-1) triangular solves (one for each tile in the i-th row of the upper triangular matrix),\n i(i-1)2 matrix multiplications to update the submatrix.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"These are the basic operations on tiles, which we are going to spawn in separate tasks in the parallel implementation. Accounting for all iterations, this makes a total of mathcalO(n^3) such tasks, decomposed as:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":" mathcalO(n) cholesky factorizations,\n mathcalO(n^2) triangular solves,\n mathcalO(n^3) matrix multiplications.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The following image illustrates the 2nd step of the algorithm:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/#Sequential-implementation","page":"Tiled Cholesky Factorization","title":"Sequential implementation","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"A sequential tiled factorization algorithm can be implemented as:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using LinearAlgebra\n\ntilerange(ti, ts) = (ti-1)*ts+1:ti*ts\n\nfunction cholesky_tiled!(A, ts)\n    m = size(A, 1); @assert m==size(A, 2)\n    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n    n = m÷ts  # number of tiles in each dimension\n\n    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n\n    for i in 1:n\n        # Diagonal cholesky serial factorization\n        cholesky!(T[i,i])\n\n        # Left tiles update\n        U = UpperTriangular(T[i,i])\n        for j in i+1:n\n            ldiv!(U', T[i,j])\n        end\n\n        # Submatrix update\n        for j in i+1:n\n            for k in j:n\n                mul!(T[j,k], T[i,j]', T[i,k], -1, 1)\n            end\n        end\n    end\n\n    # Construct the factorized object\n    return Cholesky(A, 'U', zero(LinearAlgebra.BlasInt))\nend","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Let us build a small test case to check the correctness of the factorization. Here we divide a matrix of size 4096×4096 in 8×8 tiles of size 512×512:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"n  = 4096\nts = 512\nA = rand(n, n)\nA = (A + adjoint(A))/2\nA = A + n*I;\nnothing #hide","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"and the results seem to be correct:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"F = cholesky_tiled!(copy(A), ts)\n\n# Check results\nerr = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n@show err\n@assert err < eps(Float64)","category":"page"},{"location":"examples/cholesky/cholesky/#Parallel-implementation","page":"Tiled Cholesky Factorization","title":"Parallel implementation","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"In order to parallelize the code with DataFlowTasks.jl, function calls acting on tiles are wrapped within @dspawn, along with annotations describing data access modes. We also give meaningful labels to the tasks, which will help debug and profile the code.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using DataFlowTasks\n\nfunction cholesky_dft!(A, ts)\n    m = size(A, 1); @assert m==size(A, 2)\n    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n    n = m÷ts  # number of tiles in each dimension\n\n    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n\n    for i in 1:n\n        # Diagonal cholesky serial factorization\n        @dspawn cholesky!(@RW(T[i,i])) label=\"chol ($i,$i)\"\n\n        # Left tiles update\n        U = UpperTriangular(T[i,i])\n        for j in i+1:n\n            @dspawn ldiv!(@R(U)', @RW(T[i,j])) label=\"ldiv ($i,$j)\"\n        end\n\n        # Submatrix update\n        for j in i+1:n\n            for k in j:n\n                @dspawn mul!(@RW(T[j,k]), @R(T[i,j])', @R(T[i,k]), -1, 1) label=\"schur ($j,$k)\"\n            end\n        end\n    end\n\n    # Construct the factorized object\n    r = @dspawn Cholesky(@R(A), 'U', zero(LinearAlgebra.BlasInt)) label=\"result\"\n    return fetch(r)\nend","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Again, let us check the correctness of the result:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"F = cholesky_dft!(copy(A), ts)\n\n# Check results\nerr = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n@show err\n@assert err < eps(Float64)","category":"page"},{"location":"examples/cholesky/cholesky/#Debugging-and-Profiling","page":"Tiled Cholesky Factorization","title":"Debugging and Profiling","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Let us now check what happens during a parallel run of our cholesky factorization. Thanks to the test above, the code is now compiled. Let's re-run it and collect meaningful profiling information:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"# Clean profiling environment\nGC.gc()\n\n# Real workload to be analysed\nAc = copy(A)\nlog_info = DataFlowTasks.@log cholesky_dft!(Ac, ts)","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The number of tasks being mathcalO(n^3), we can see how quickly the DAG complexity increases (even though the test case only has 8×8 tiles here):","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using GraphViz\ndag = GraphViz.Graph(log_info)\nnothing #hide","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The critical path, highlighted in red, includes all cholesky factorizations of diagonal tiles, as well as the required tasks in between them.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The parallel trace plot gives us more details about the performance limiting factors:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using CairoMakie # or GLMakie in order to have more interactivity\ntrace = plot(log_info; categories=[\"chol\", \"ldiv\", \"schur\"])\nnothing #hide","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The overhead incurred by DataFlowTasks seems relatively small here: the time taken inserting tasks is barely measurable, and the scheduling did not lead to threads waiting idly for too long. This is confirmed by the \"Time Bounds\" plot, showing a measured wall clock time not too much longer than the lower bound obtained when suppressing idle time.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The \"Times per Category\" plot seems to indicate that the matrix multiplications performed in the \"Schur\" tasks account for the majority of the computing time. Trying to optimize these would be a priority to increase the sequential performance of the factorization.","category":"page"},{"location":"examples/cholesky/cholesky/#Performances","page":"Tiled Cholesky Factorization","title":"Performances","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The performance of this example can be improved by using better implementations for the sequential building blocks operating on tiles:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"LoopVectorization.jl can improve the performance of the sequential cholesky factorization of diagonal blocks as well as the schur_complement\nTriangularSolve.jl provides a high-performance ldiv! implementation","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"This approach is pursued in TiledFactorization.jl, where all the above mentioned building blocks are combined with the parallelization strategy presented here to create a pure Julia implementation of the matrix factorizations. The performances of this implementation is assessed in the following plot, by comparison to MKL on a the case of a 5000x5000 matrix decomposed in tiles of size 256x256.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The figure above was generated by running this script on a machine with 2x10 Intel Xeon Silver 4114 cores (2.20GHz) with the following topology:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/#references-section","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Modules = [DataFlowTasks, GraphViz_Ext, Makie_Ext]","category":"page"},{"location":"references/#DataFlowTasks.DataFlowTasks","page":"References","title":"DataFlowTasks.DataFlowTasks","text":"module DataFlowTask\n\nCreate Tasks which can keep track of how data flows through it.\n\n\n\n\n\n","category":"module"},{"location":"references/#DataFlowTasks.LOGINFO","page":"References","title":"DataFlowTasks.LOGINFO","text":"const LOGINFO::Ref{LogInfo}\n\nGlobal LogInfo being used to record the events. Can be changed using _setloginfo!.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.SCHEDULER","page":"References","title":"DataFlowTasks.SCHEDULER","text":"const SCHEDULER::Ref{TaskGraphScheduler}\n\nThe active scheduler being used.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.TASKCOUNTER","page":"References","title":"DataFlowTasks.TASKCOUNTER","text":"const TASKCOUNTER::Ref{Int}\n\nGlobal counter of created DataFlowTasks.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.AccessMode","page":"References","title":"DataFlowTasks.AccessMode","text":"@enum AccessMode READ WRITE READWRITE\n\nDescribe how a DataFlowTask access its data.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.DAG","page":"References","title":"DataFlowTasks.DAG","text":"struct DAG{T}\n\nRepresentation of a directed acyclic graph containing nodes of type T. The list of nodes with edges coming into a node i can be retrieved using inneighbors(dag,i); similarly, the list of nodes with edges leaving from i can be retrieved using outneighbors(dag,i).\n\nDAG is a buffered structure with a buffer of size sz_max: calling addnode! on it will block if the DAG has more than sz_max elements.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.DAG-Union{Tuple{}, Tuple{Any}, Tuple{T}} where T","page":"References","title":"DataFlowTasks.DAG","text":"DAG{T}(sz)\n\nCreate a buffered DAG holding a maximum of sz nodes of type T.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.DataFlowTask","page":"References","title":"DataFlowTasks.DataFlowTask","text":"DataFlowTask(func,data,mode)\n\nCreate a task-like object similar to Task(func) which accesses data with AccessMode mode.\n\nWhen a DataFlowTask is created, the elements in its data field will be checked against all other active DataFlowTask to determined if a dependency is present based on a data-flow analysis. The resulting Task will then wait on those dependencies.\n\nA DataFlowTask behaves much like a Julia Task: you can call wait(t), schedule(t) and fetch(t) on it.\n\nSee also: @dtask, @dspawn, @dasync.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.FinishedChannel","page":"References","title":"DataFlowTasks.FinishedChannel","text":"struct FinishedChannel{T} <: AbstractChannel{T}\n\nUsed to store tasks which have been completed, but not yet removed from the underlying DAG. Taking from an empty FinishedChannel will block.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.InsertionLog","page":"References","title":"DataFlowTasks.InsertionLog","text":"struct InsertionLog\n\nLogs the execution trace of a DataFlowTask insertion.\n\nFields:\n\ntime_start  : time the insertion began\ntime_finish : time the insertion finished\ntaskid      : the task it is inserting\ntid         : the thread on which the insertion is happening\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.JuliaScheduler","page":"References","title":"DataFlowTasks.JuliaScheduler","text":"struct JuliaScheduler{T} <: TaskGraphScheduler{T}\n\nImplements a simple scheduling strategy which consists of delegating the DataFlowTasks to the native Julia scheduler for execution immediately after the data dependencies have been analyzed using its dag::DAG. This is the default scheduler used by DataFlowTasks.\n\nThe main advantage of this strategy is its simplicity and composability. The main disadvantage is that there is little control over how the underlying Tasks are executed by the Julia scheduler (e.g., no priorities can be passed).\n\nCalling JuliaScheduler(sz) creates a new scheduler with an empty DAG of maximum capacity sz.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.LogInfo","page":"References","title":"DataFlowTasks.LogInfo","text":"struct LogInfo\n\nContains informations on the program's progress. For thread-safety, the LogInfo structure uses one vector of TaskLog per thread.\n\nYou can visualize and postprocess a LogInfo using GraphViz.Graph and Makie.plot.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.Stop","page":"References","title":"DataFlowTasks.Stop","text":"struct Stop\n\nSingleton type used to safely interrupt a task reading from an AbstractChannel.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.TaskGraph","page":"References","title":"DataFlowTasks.TaskGraph","text":"const TaskGraph = DAG{DataFlowTask}\n\nA directed acyclic graph of DataFlowTasks.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.TaskGraphScheduler","page":"References","title":"DataFlowTasks.TaskGraphScheduler","text":"abstract type TaskGraphScheduler\n\nStructures implementing a strategy to evaluate a DAG.\n\nConcrete subtypes are expected to contain a dag::DAG field for storing the task graph, and a finished::AbstractChannel field to keep track of completed tasks. The interface requires the following methods:\n\n-spawn(t,sch) -schedule(t,sch)\n\nSee also: JuliaScheduler\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.TaskLog","page":"References","title":"DataFlowTasks.TaskLog","text":"struct TaskLog\n\nLogs the execution trace of a DataFlowTask.\n\nFields:\n\ntag         : task id in DAG\ntime_start  : time the task started running\ntime_finish : time the task finished running\ntid         : thread on which the task ran\ninneighbors : vector of incoming neighbors in DAG\nlabel       : a string used for displaying and/or postprocessing tasks\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks._getloginfo-Tuple{}","page":"References","title":"DataFlowTasks._getloginfo","text":"_getloginfo()\n\nReturn the active logger.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks._setloginfo!-Tuple{Union{Nothing, DataFlowTasks.LogInfo}}","page":"References","title":"DataFlowTasks._setloginfo!","text":"_setloginfo!(l::LogInfo)\n\nSet the active logger to l.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.access_mode-Tuple{DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.access_mode","text":"access_mode(t::DataFlowTask[,i])\n\nHow t accesses its data.\n\nSee: AccessMode\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addedge!-Union{Tuple{T}, Tuple{DataFlowTasks.DAG{T}, T, T}} where T","page":"References","title":"DataFlowTasks.addedge!","text":"addedge!(dag,i,j)\n\nAdd (directed) edge connecting node i to node j in the dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addedge_transitive!-Tuple{Any, Any, Any}","page":"References","title":"DataFlowTasks.addedge_transitive!","text":"addedge_transitive!(dag,i,j)\n\nAdd edge connecting nodes i and j if there is no path connecting them already.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addnode!-Union{Tuple{T}, Tuple{DataFlowTasks.DAG{T}, T}, Tuple{DataFlowTasks.DAG{T}, T, Any}} where T","page":"References","title":"DataFlowTasks.addnode!","text":"addnode!(dag,(k,v)::Pair[, check=false])\naddnode!(dag,k[, check=false])\n\nAdd a node to the dag. If passed only a key k, the value v is initialized as empty (no edges added). The check flag is used to indicate if a data flow analysis should be performed to update the dependencies of the newly inserted node.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.capacity-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.capacity","text":"capacity(dag)\n\nThe maximum number of nodes that dag can contain.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.data-Tuple{DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.data","text":"data(t::DataFlowTask[,i])\n\nData accessed by t.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.data_dependency-Tuple{DataFlowTasks.DataFlowTask, DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.data_dependency","text":"data_dependency(t1::DataFlowTask,t1::DataFlowTask)\n\nDetermines if there is a data dependency between t1 and t2 based on the data they read from and write to.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.enable_debug","page":"References","title":"DataFlowTasks.enable_debug","text":"enable_debug(mode = true)\n\nIf mode is true (the default), enable debug mode: errors inside tasks will be shown.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.enable_log","page":"References","title":"DataFlowTasks.enable_log","text":"enable_log(mode = true)\n\nIf mode is true (the default), logging is enabled throug the @log macro. Calling enable_log(false) will de-activate logging at compile time to avoid any possible overhead.\n\nNote that changing the log mode at runtime will may invalidate code, possibly triggering recompilation.\n\nSee also: @log, with_logging\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.force_linear_dag","page":"References","title":"DataFlowTasks.force_linear_dag","text":"force_linear_dag(mode=false)\n\nIf mode is true, nodes are added to the DAG in a linear fashion, i.e. the DAG connects node i to node i+1. This is useful for debugging purposes.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.force_sequential","page":"References","title":"DataFlowTasks.force_sequential","text":"force_sequential(mode = true)\n\nIf mode is true, enable sequential mode: no tasks are created and scheduled, code is simply run as it appears in the sources. In effect, this makes @dspawn a no-op.\n\nBy default, sequential mode is disabled when the program starts.\n\nSee also: force_linear_dag.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.getscheduler-Tuple{}","page":"References","title":"DataFlowTasks.getscheduler","text":"getscheduler()\n\nReturn the active scheduler.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.has_edge-Tuple{DataFlowTasks.DAG, Any, Any}","page":"References","title":"DataFlowTasks.has_edge","text":"has_edge(dag,i,j)\n\nCheck if there is an edge connecting i to j.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.inneighbors-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.inneighbors","text":"inneighbors(dag,i)\n\nList of predecessors of i in dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.isconnected-Tuple{DataFlowTasks.DAG, Any, Any}","page":"References","title":"DataFlowTasks.isconnected","text":"isconnected(dag,i,j)\n\nCheck if there is a path in dag connecting i to j.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.memory_overlap-Tuple{Any, Any}","page":"References","title":"DataFlowTasks.memory_overlap","text":"memory_overlap(di,dj)\n\nDetermine if data di and dj have overlapping memory in the sense that mutating di can change dj (or vice versa). This function is used to build the dependency graph between DataFlowTasks.\n\nA generic version is implemented returning true (but printing a warning). Users should overload this function for the specific data types used in the arguments to allow for appropriate inference of data dependencies.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.memory_overlap-Tuple{Array, Array}","page":"References","title":"DataFlowTasks.memory_overlap","text":"memory_overlap(di::AbstractArray,dj::AbstractArray)\n\nTry to determine if the arrays di and dj have overlapping memory.\n\nWhen both di and dj are Arrays of bitstype, simply compare their addresses. Otherwise, compare their parents by default.\n\nWhen both di and dj are SubArrays we compare the actual indices of the SubArrays when their parents are the same (to avoid too many false positives).\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.nodes-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.nodes","text":"nodes(dag::DAG)\n\nReturn an iterator over the nodes of dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.num_edges-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.num_edges","text":"num_edges(dag::DAG)\n\nNumber of edges in the DAG.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.num_nodes-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.num_nodes","text":"num_nodes(dag::DAG)\n\nNumber of nodes in the DAG.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.outneighbors-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.outneighbors","text":"outneighbors(dag,i)\n\nList of successors of j in dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.remove_node!-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.remove_node!","text":"remove_node!(dag::DAG,i)\n\nRemove node i and all of its edges from dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.restart_scheduler!-Tuple{}","page":"References","title":"DataFlowTasks.restart_scheduler!","text":"restart_scheduler!([sch])\n\nInterrupt all tasks in sch and remove them from the underlying DAG.\n\nThis function is useful to avoid having to restart the REPL when a task in sch errors.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.savedag","page":"References","title":"DataFlowTasks.savedag","text":"DataFlowTasks.savedag(filepath, graph)\n\nSave graph as an SVG image at filepath. This requires GraphViz to be available.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.setscheduler!-Tuple{Any}","page":"References","title":"DataFlowTasks.setscheduler!","text":"setscheduler!(sch)\n\nSet the active scheduler to sch.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.start_dag_worker","page":"References","title":"DataFlowTasks.start_dag_worker","text":"start_dag_worker(sch)\n\nStart a task associated with sch which takes nodes from its finished queue and removes them from the dag. The task blocks if finished is empty.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.sync","page":"References","title":"DataFlowTasks.sync","text":"sync([sch::TaskGraphScheduler])\n\nWait for all nodes in sch to be finished before continuining. If called with no arguments, use the current scheduler.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.update_edges!-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.update_edges!","text":"update_edges!(dag::DAG,i)\n\nPerform the data-flow analysis to update the edges of node i. Both incoming and outgoing edges are updated.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_logging!-Tuple{Any, DataFlowTasks.LogInfo}","page":"References","title":"DataFlowTasks.with_logging!","text":"with_logging!(f,l::LogInfo)\n\nSimilar to with_logging, but append events to l.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_logging-Tuple{Any}","page":"References","title":"DataFlowTasks.with_logging","text":"with_logging(f) --> f(),loginfo\n\nExecute f() and log DataFlowTasks into the loginfo object.\n\nExamples:\n\nusing DataFlowTasks\n\nA,B = zeros(2), ones(2);\n\nout,loginfo = DataFlowTasks.with_logging() do\n    @dspawn fill!(@W(A),1)\n    @dspawn fill!(@W(B),1)\n    res = @dspawn sum(@R(A)) + sum(@R(B))\n    fetch(res)\nend\n\n#\n\nout\n\nSee also: LogInfo\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_scheduler-Tuple{Any, Any}","page":"References","title":"DataFlowTasks.with_scheduler","text":"with_scheduler(f,sch)\n\nRun f, but push DataFlowTasks to the scheduler dag in sch instead of the default dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.@dasync-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@dasync","text":"@dasync expr [kwargs...]\n\nLike @dspawn, but schedules the task to run on the current thread.\n\nSee also:\n\n@dspawn, @dtask\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@dspawn-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@dspawn","text":"@dspawn expr [kwargs...]\n\nCreate a DataFlowTask to execute the code given by expr, and schedule it to run on any available thread. The code in expr should be annotated with @R, @W and/or @RW tags in order to indicate how it accesses data (see examples below). This information is is then used to automatically infer task dependencies.\n\nSupported keyword arguments:\n\nlabel: provide a label to identify the task. This is useful when logging scheduling information;\npriority: inform the scheduler about the relative priority of the task. This information is not (yet) leveraged by the default scheduler.\n\nSee also: @dtask, @dasync\n\nExamples:\n\nBelow are 3 equivalent ways to create the same DataFlowTask, which expresses a Read-Write dependency on C and Read dependencies on A and B\n\nusing DataFlowTasks,LinearAlgebra\nA = ones(5, 5)\nB = ones(5, 5)\nC = zeros(5, 5)\nα, β = (1, 0)\n\n# Option 1: annotate arguments in a function call\n@dspawn mul!(@RW(C), @R(A), @R(B), α, β)\n\n# Option 2: specify data access modes in the code block\n@dspawn begin\n   @RW C\n   @R  A B\n   mul!(C, A, B, α, β)\nend\n\n# Option 3: specify data access modes after the code block\n# (i.e. alongside keyword arguments)\nres = @dspawn mul!(C, A, B, α, β) @RW(C) @R(A,B)\n\nfetch(res) # a 5×5 matrix of 5.0\n\nHere is a more complete example, demonstrating a full computation involving 2 different tasks.\n\nusing DataFlowTasks\n\nA = rand(5)\n\n# create a task with WRITE access mode to A\n# and label \"writer\"\nt1 = @dspawn begin\n    @W A\n    sleep(1)\n    fill!(A,0)\n    println(\"finished writing\")\nend  label=\"writer\"\n\n# create a task with READ access mode to A\nt2 = @dspawn begin\n    @R A\n    println(\"I automatically wait for `t1` to finish\")\n    sum(A)\nend  priority=1\n\nfetch(t2) # 0\n\n# output\n\nfinished writing\nI automatically wait for `t1` to finish\n0.0\n\nNote that in the example above t2 waited for t1 because it read a data field that t1 accessed in a writable manner.\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@dtask-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@dtask","text":"@dtask expr [kwargs...]\n\nCreate a DataFlowTask to execute expr, where data have been tagged to specify how they are accessed. Note that the task is not automatically scheduled for execution.\n\nSee @dspawn for information on how to annotate expr to specify data dependencies, and a list of supported keyword arguments.\n\nSee also: @dspawn, @dasync\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@log-Tuple{Any}","page":"References","title":"DataFlowTasks.@log","text":"DataFlowTasks.@log expr --> LogInfo\n\nExecute expr and return a LogInfo instance with the recorded events.\n\nwarning: Warning\nThe returned LogInfo instance may be incomplete if block returns before all DataFlowTasks spawened inside of it are completed. Typically expr should fetch the outcome before returning to properly benchmark the code that it runs (and not merely the tasks that it spawns).\n\nSee also: with_logging, with_logging!\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@using_opt-Tuple{Any}","page":"References","title":"DataFlowTasks.@using_opt","text":"DataFlowTasks.@using_opt pkgnames\n\nLoad pkgnames from optional dependencies.\n\nwarning: Warning\nThis feature is experimental and might break in the future. It may be useful for quick experiments, but adding GraphViz or Makie to a full-fledged environment should be preferred when possible.\n\nExamples:\n\nusing DataFlowTasks\nDataFlowTasks.@using_opt GraphViz\n\n\n\n\n\n","category":"macro"},{"location":"references/#GraphViz.Graph-Tuple{DataFlowTasks.LogInfo}","page":"References","title":"GraphViz.Graph","text":"GraphViz.Graph(log_info::LogInfo)\n\nProduce a GraphViz.Graph representing the DAG of tasks collected in log_info.\n\nSee also: DataFlowTasks.@log\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks_GraphViz_Ext.loggertodot-Tuple{Any}","page":"References","title":"DataFlowTasks_GraphViz_Ext.loggertodot","text":"loggertodot(logger)  --> dagstring\n\nReturn a string in the DOT format representing the underlying graph in logger and to be plotted by GraphViz with Graph(loggertodot())\n\n\n\n\n\n","category":"method"},{"location":"references/#MakieCore.plot-Tuple{DataFlowTasks.LogInfo}","page":"References","title":"MakieCore.plot","text":"plot(log_info; categories)\n\nPlot DataFlowTasks log_info labeled informations with categories.\n\nEntries in categories define how to group tasks in categories for plotting. Each entry can be:\n\na String: in this case, all tasks having labels in which the string occurs are grouped together. The string is also used as a label for the category itself.\na String => Regex pair: in this case, all tasks having labels matching the regex are grouped together. The string is used as a label for the category itself.\n\nSee the documentation for more information on how to profile and visualize DataFlowTasks.\n\n\n\n\n\n","category":"method"},{"location":"examples/hmat/hmat/#Hierarchical-LU-factorization","page":"Hierarchical LU factorization","title":"Hierarchical LU factorization","text":"","category":"section"},{"location":"examples/lu/lu/#tiledlu-section","page":"Tiled LU factorization","title":"Tiled LU factorization","text":"","category":"section"},{"location":"examples/lu/lu/","page":"Tiled LU factorization","title":"Tiled LU factorization","text":"tip: Tip\nSee this page for a discussion on thread-based parallelization of LU factorization.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"EditURL = \"https://github.com/maltezfaria/DataFlowTasks.jl/blob/main/docs/src/examples/blur-roberts/blur-roberts.jl\"","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Blur-and-Roberts-image-filters","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"(Image: ipynb) (Image: nbviewer)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"This example illustrate the use of DataFlowTasks.jl to parallelize the tiled application of two kernels used in image processing. The application first applies a blur filter on each pixel of the image; in a second step, the Roberts cross operator is applied to detect edges in the image.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Let us first load a test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using Images\nurl = \"https://upload.wikimedia.org/wikipedia/commons/c/c3/Equus_zebra_hartmannae_-_Etosha_2015.jpg\"\nispath(\"test-image.jpg\") || download(url, \"test-image.jpg\")\nimg = Gray.(load(\"test-image.jpg\"))","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"We start by defining a few helper functions:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"the contract and expand functions manipulate ranges of indices in order to respectively contract or expand them by a few pixels;\nthe img2mat and mat2img convert between a Gray-scale image and a matrix of floating-point pixel intensities. The filters will work on this latter representation, which may need a renormalization to be converted back to a Gray-scale image.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"contract(range,n) = range[begin+n:end-n]\nexpand(range,n)   = range[begin]-n:range[end]-n\n\nfunction img2mat(img)\n    PixelType = eltype(img)\n    mat = Float64.(img)\n    return (PixelType, mat)\nend\n\nfunction mat2img(PixelType, mat)\n    m1, m2 = extrema(mat)\n    PixelType.((mat .- m1) ./ (m2-m1))\nend\n\nPixelType, mat = img2mat(img);\nnothing #hide","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Filters-implementation","page":"Blur & Roberts image filters","title":"Filters implementation","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The blur! function averages the value of each pixel with the values of all pixels less than width pixels away in manhattan distance. In order to simplify the implementation, the filter is applied only to pixels that are sufficiently far from the boundary to have all their neighbors correctly defined.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Results are written in-place in a pre-allocated dest array. Unless otherwise specified, the filter is applied to the whole image, but can be reduced to a tile if a smaller range argument is provided.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur!(dest, src; range=axes(src), width)\n    ri, rj = intersect.(range, contract.(axes(src), width))\n\n    weight = 1/(2*width+1)^2\n    @inbounds for i in ri, j in rj\n        dest[i,j] = 0\n        for δi in -width:width, δj in -width:width\n            dest[i,j] += src[i+δi, j+δi]\n        end\n        dest[i,j] *= weight\n    end\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"In the following, we'll use a filter width of 5 pixels, which produces the following results on the test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"width = 5\nblurred = similar(mat)\n\nblur!(blurred, mat; width)\n\nmat2img(PixelType, blurred)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The roberts! function applies the Roberts cross operator to the provided image. Like above, it operates by default on all pixels in the image (provided they are sufficiently far from the boundaries), but can be restricted to work on a tile if the range argument is provided.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function roberts!(dest, src; range=axes(src))\n    ri, rj = intersect.(range, contract.(axes(src), 1))\n\n    for i in ri, j in rj\n        dest[i,j] = (\n            + (sqrt(src[i,  j]) - sqrt(src[i+1,j+1]))^2\n            + (sqrt(src[i+1,j]) - sqrt(src[i  ,j+1]))^2\n        )^(0.25)\n    end\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Applying this edge detection filter on the original image produces the following results:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"contour = similar(mat)\nroberts!(contour, mat)\n\nmat2img(PixelType, contour)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Chaining the blur and roberts filters may make edge detection less noisy:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur_roberts!(img; width, tmp=similar(img))\n    blur!(tmp, img; width)\n    roberts!(img, tmp)\nend\n\nmat1 = copy(mat)\ntmp  = similar(mat)\n\nblur_roberts!(mat1; width, tmp)\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Tiled-filter-application","page":"Blur & Roberts image filters","title":"Tiled filter application","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The TiledIteration.jl package implements various tools allowing to define and iterate over disjoint tiles of a larger array. We'll use it to apply the filters tile by tile.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The map_tiled! higher-order function automates the application of a filter fun! on all pixels of an image src decomposed with a tilesize ts. This higher-order function is then used to defined tiled versions of the blur and roberts filters.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using TiledIteration\n\nfunction map_tiled!(fun!, dest, src, ts)\n    for tile in TileIterator(axes(src), (ts, ts))\n        fun!(dest, src, tile)\n    end\nend\n\nblur_tiled!(dest, src, ts; width) = map_tiled!(dest, src, ts) do dest, src, tile\n    blur!(dest, src; width, range=tile)\nend\n\nroberts_tiled!(dest, src, ts) = map_tiled!(dest, src, ts) do dest, src, tile\n    roberts!(dest, src; range=tile)\nend\n\nfunction blur_roberts_tiled!(img, ts; width, tmp=similar(img))\n    blur_tiled!(tmp, img, ts; width)\n    roberts_tiled!(img, tmp, ts)\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Decomposing the original image in tiles of size 512times 512, the tiled application of the filters yields the same result as above, in a more cache-efficient way:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"ts = 512\n\nmat1 .= mat\nblur_roberts_tiled!(mat1, ts; width, tmp)\n\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Parallel-filter-application","page":"Blur & Roberts image filters","title":"Parallel filter application","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Parallelizing the tiled filter application is relatively straightforward using DataFlowTasks.jl. As usual, it involves specifying which data is accessed by each task.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using DataFlowTasks\n\nfunction blur_dft!(dest, src, ts; width)\n    map_tiled!(dest, src, ts) do dest, src, tile\n        outer = intersect.(expand.(tile, width), axes(src))\n        @dspawn begin\n            @R view(src, outer...)\n            @W view(dest, tile...)\n            blur!(dest, src; width, range=tile)\n        end label=\"blur ($tile)\"\n    end\n    @dspawn @R(dest) label=\"blur (result)\"\nend\n\nfunction roberts_dft!(dest, src, ts)\n    map_tiled!(dest, src, ts) do dest, src, tile\n        outer = intersect.(expand.(tile, 1), axes(src))\n        @dspawn begin\n            @R view(src, outer...)\n            @W view(dest, tile...)\n            roberts!(dest, src; range=tile)\n        end label=\"roberts ($tile)\"\n    end\n    @dspawn @R(dest) label=\"roberts (result)\"\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Note how each filter spawns one task for each tile, and an extra task to get the results in the end. This allows applying a given filter independently of the other.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"However, the filters remain composable: when applying both filters one after the other, no implicit synchronization is enforced at the end of the blurring stage, and the runtime may decide to intersperse blurring and roberts tasks (as long as the blurring of a tile and all its neighbors is performed before the application of the roberts filter on this tile).","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur_roberts_dft!(img, ts; width, tmp=similar(img))\n    blur_dft!(tmp, img, ts; width)\n    roberts_dft!(img, tmp, ts)\n    @dspawn @R(img) label=\"result\"\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Again this yields the same results on the test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"mat1 .= mat;\nblur_roberts_dft!(mat1, ts; width, tmp) |> wait\n\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Profiling-the-parallel-version","page":"Blur & Roberts image filters","title":"Profiling the parallel version","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"As usual, profiling data should be collected in a context that is as clean as possible.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"GC.gc()\n\nmat1 .= mat;\nlog_info = DataFlowTasks.@log wait(blur_roberts_dft!(mat1, ts; width, tmp))","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The parallel trace shows how blur and roberts tasks are interspersed in the time line:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"DataFlowTasks.@using_opt CairoMakie\n\ntrace = plot(log_info, categories=[\"blur\", \"roberts\"])\nnothing #hide","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"(Image: )","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"In terms of performance, elapsed time seems to be bounded in this case by the total computing time of all threads. Re-running the same computation with more threads may help reduce the overall wall-clock time.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"This page was generated using Literate.jl.","category":"page"},{"location":"issues/#issues-section","page":"Common Issues","title":"Common Issues","text":"","category":"section"},{"location":"issues/","page":"Common Issues","title":"Common Issues","text":"time(Critical Path) = 0 : there's no dependencies between tasks (or you didn't give any, or it didn't work the way it was supposed to).\nHuge Other times : the DataFlowTasks.resetlogger!() was probably forgotten, hence the first measurerd time currently in the logger's memory is one from a previous run.\nMention let block discussions","category":"page"},{"location":"examples/stencil/stencil/#Iterative-Stencil-Loop","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"","category":"section"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"The Gauss-Seidel algorithm uses the following five-points stencil :","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"v_ij^n+1 = frac15(v_ij-1^n+1 + v_ij-1^n+1 + v_ij^n + v_i+1j^n + v_ij+1^n)","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"where the element (ij) of the matrix v at time n+1 depend on values of the same iteration n+1. Those dependencies might be tricky to track, but, at each update, we know which elements it needs. So we know the access modes of the data.","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"We'll work on a matrix of size (N+2)*(N+2), with N*N real elements and a halo of zero surrounding them to simplify boundary conditions.","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"The update of a tile of the matrix is given by the next function :","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"# Update a tile of v composed by elements at index in range (ri,rj)\nfunction step!(v, ri, rj)\n    for i ∈ ri, j ∈ rj\n        v[i,j] = 1/5 * (v[i-1,j] + v[i,j-1] + v[i,j] + v[i+1,j] + v[i,j+1])\n    end\nend","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"The sequential core of the algorithm by block will then be :","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"tilerange(ti, ts) = (ti-1)*ts+2:ti*ts+1\nfunction gauss_seidel(v)\n    m,n = size(v).-2\n    tn = round(Int, n/ts)\n    for ti ∈ 1:tn, tj ∈ 1:tn\n        (ri,rj) = tilerange.([ti tj], ts)\n        step!(v, ri, rj)\n    end\nend","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"And the parallelization with DataFlowTasks :","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"using DataFlowTasks\nimport DataFlowTasks as DFT\n\nfunction core!(v, ts)\n    m,n = size(v).-2\n    tn = round(Int, n/ts)\n\n    for ti ∈ 1:tn, tj ∈ 1:tn\n        (ri,rj) = tilerange.([ti tj], ts)\n        @dspawn begin\n            @R view(v, ri[1]-1, rj)\n            @R view(v, ri[end]+1, rj) \n            @R view(v, ri, rj[1]-1)\n            @R view(v, ri, rj[end]+1)\n            @RW view(v, ri, rj)\n            step!(v, ri, rj)\n        end label=\"tile ($ti,$tj)\"\n    end\n    DFT.sync()\nend","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"The final peaces of the code :","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"using GLMakie, GraphViz\n\nfunction main()\n    # Declaration\n    N = 4096            # Nb of elements\n    ts = 512            # Tilesize\n    v = zeros(N+2,N+2)  # Matrix\n    initialization!(v)  # Init a choc at the center\n    DFT.enable_log()\n\n    # Compilation\n    gauss_seidel(copy(v), ts)\n\n    # Reset DFT Environnement\n    DFT.reset!()\n\n    # Call\n    gauss_seidel(v, ts)\n\n    # Profile\n    DFT.plot(categories=[\"tile\"])\nend","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"The profiling gives us the next results.","category":"page"},{"location":"examples/stencil/stencil/","page":"Iterative Stencil Loop","title":"Iterative Stencil Loop","text":"(Image: GaussTrace) (Image: GaussDag)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"CurrentModule = DataFlowTasks","category":"page"},{"location":"#DataFlowTasks","page":"Getting started","title":"DataFlowTasks","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Tasks which automatically respect data-flow dependencies","category":"page"},{"location":"#Basic-usage","page":"Getting started","title":"Basic usage","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"This package defines a DataFlowTask type which behaves very much like a Julia native Task, except that it allows the user to specify explicit data dependencies. This information is then be used to automatically infer task dependencies by constructing and analyzing a directed acyclic graph based on how tasks access the underlying data. The premise is that it is sometimes simpler to specify how tasks depend on data than to specify how tasks depend on each other.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The use of a DataFlowTask object is intended to be as similar as possible to a Julia native Task. The API implements three macros :","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"@dspawn\n@dtask\n@dasync","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"which behave like their Base counterparts, except they take additional annotations that declare how each task affects the data it accesses:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"read-only: @R or @READ\nwrite-only: @W or @WRITE\nread-write: @RW or @READWRITE","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Anywhere in the task body, a @R(A) annotation for example implies that data A will be accessed in read-only mode by the task.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks # hide\n\nA = ones(5)\nB = ones(5)\nd = @dspawn begin\n    @RW A   # A is accessed in READWRITE mode\n    @R  B   # B is accessed in READ mode\n    A .= A .+ B\nend\n\nfetch(d)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"This creates (and schedules for execution) a DataFlowTask d which accesses A in READWRITE mode, and B in READ mode. The benefit of DataFlowTasks comes when you start to compose operations which may mutate the same data:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks # hide\n\nn = 100_000\nA = ones(n)\n\nd1 = @dspawn begin\n    @RW A\n\n    # in-place work on A\n    for i in eachindex(A)\n        A[i] = log(A[i]) # A[i] = 0\n    end\nend\n\n# reduce A\nd2 = @dspawn sum(@R A)\n# The above is a shortcut for:\n#   d2 = @dspawn begin\n#       @R A\n#       sum(A)\n#   end\n\nc = fetch(d2) # 0","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We now have two asynchronous tasks being created, both of which access the array A. Because d1 writes to A, and d2 reads from it, the outcome C is nondeterministic unless we specify an order of precedence. DataFlowTasks reinforces the sequential consistency criterion, which is to say that executing tasks in parallel must preserve, up to rounding errors, the result that would have been obtained if they were executed sequentially (i.e. d1 is executed before d2, d2 before d3, and so on). In this example, this means d2 will always wait on d1 because of an inferred data dependency. The outcome is thus always zero.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nIf you replace @dspawn by Threads.@spawn in the example above (and pick an n large enough) you will see that you no longer get 0 because d2 may access an element of A before it has been replaced by zero!","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nIn the d2 example above, a shortcut syntax was introduced, which allows putting access mode annotations directly around arguments in a function call. This is especially useful when the task body is a one-liner.See @dspawn for an exhaustive list of supported ways to create tasks and specify data dependencies.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"No parallelism was allowed in the previous example due to a data conflict. To see that when parallelism is possible, DataFlowTasks will exploit it, consider this one last example:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks # hide\n\nn = 100\nA = ones(n)\n\nd1 = @dspawn begin\n    @W A\n\n    # write to A\n    sleep(1)\n    fill!(A,0)\nend\n\nd2 = @dspawn begin\n    @R A\n\n    # some long computation \n    sleep(5)\n    # reduce A\n    sum(A)\nend\n\n# another reduction on A\nd3 = @dspawn sum(x->sin(x), @R(A))\n\nt = @elapsed c = fetch(d3)\n\nt,c ","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We see that the elapsed time to fetch the result from d3 is on the order of one second. This is expected since d3 needs to wait on d1 but can be executed concurrently with d2. The result is, as expected, 0.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"All examples this far have been simple enough that the dependencies between the tasks could have been inserted by hand. There are certain problems, however, where the constant reuse of memory (mostly for performance reasons) makes a data-flow approach to parallelism a rather natural way to implicitly describe task dependencies. This is the case, for instance, of tiled (also called blocked) matrix factorization algorithms, where task dependencies can become rather difficult to describe in an explicit manner. The tiled factorization section showcases some non-trivial problems for which DataFlowTasks may be useful.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nThe main goal of DataFlowTasks is to expose parallelism: two tasks ti and tj can be executed concurrently if one does not write to memory that the other reads. This data-dependency check is done dynamically, and therefore is not limited to tasks in the same lexical scope. Of course, there is an overhead associated with these checks, so whether performance gains can be obtained depend largely on how parallel the algorithm is, as well as how long each individual task takes (compared to the overhead).","category":"page"},{"location":"#Custom-types","page":"Getting started","title":"Custom types","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"In order to infer dependencies between DataFlowTasks, we must be able to determine whether two objects A and B share a common memory space. That is to say, we must know if mutating A can affect B, or vice-versa. Obviously, without any further information on the types of A and B, this is an impossible question.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"To get around this challenge, you must import and extend the memory_overlap method to work on any pair of elements A and B that you wish to use. The examples in the previous section worked because these methods have been defined for some basic AbstractArrays:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: memory_overlap\n\nA = rand(10,10)\nB = view(A,1:10)\nC = view(A,11:20)\n\nmemory_overlap(A,B),memory_overlap(A,C),memory_overlap(B,C)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"By default, memory_overlap will return true and print a warning if it does not find a specialized method:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: memory_overlap\n\nstruct CirculantMatrix\n    data::Vector{Float64}\nend\n\nv = rand(10);\nM = CirculantMatrix(v);\n\nmemory_overlap(M,copy(v))","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Extending the memory_overlap will remove the warning, and produce a more meaningful result:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"import DataFlowTasks: memory_overlap\n\n# overload the method\nmemory_overlap(M::CirculantMatrix,v) = memory_overlap(M.data,v)\nmemory_overlap(v,M::CirculantMatrix) = memory_overlap(M,v)\n\nmemory_overlap(M,v), memory_overlap(M,copy(v))","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"You can now spawn tasks with your custom type CirculantMatrix as a data dependency, and things should work as expected:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks\n\nv  = ones(5);\nM1 = CirculantMatrix(v);\nM2 = CirculantMatrix(copy(v));\n\nBase.sum(M::CirculantMatrix) = length(M.data)*sum(M.data)\n\nd1 = @dspawn begin\n    @W v\n    sleep(0.5)\n    fill!(v,0) \nend;\nd2 = @dspawn sum(@R M1)\nd3 = @dspawn sum(@R M2)\n\nfetch(d3) # 25\n\nfetch(d2) # 0","category":"page"},{"location":"#Scheduler","page":"Getting started","title":"Scheduler","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"When loaded, the DataFlowTasks package will initialize an internal scheduler (of type JuliaScheduler), running on the background, to handle implicit dependencies of the spawned DataFlowTasks. In order to retrieve the current scheduler, you may use the getscheduler method:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks # hide\nDataFlowTasks.sync() # hide\nsch = DataFlowTasks.getscheduler()","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The default scheduler can be changed through setscheduler!.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"There are two important things to know about the default JuliaScheduler type. First, it contains a buffered DAG that can handle up to sz_max nodes: trying to spawn a task when the DAG is full will block. This is done to keep the cost of analyzing the data dependencies under control, and it means that a full/static DAG may in practice never be constructed. You can modify the buffer size as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"resize!(sch.dag,50)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Second, when the computation of a DataFlowTask ti is completed, it gets pushed into a finished channel, to be eventually processed and poped from the DAG by the dag_worker. This is done to avoid concurrent access to the DAG: only the dag_worker should modify it. If you want to stop nodes from being removed from the DAG, you may stop the dag_worker using:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"DataFlowTasks.stop_dag_worker(sch)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Finished nodes will now remain in the DAG:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: R,W,RW, num_nodes\nA = ones(5)\n@dspawn begin \n    @RW A\n    A .= 2 .* A\nend\n@dspawn sum(@R A)\nsch","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that stopping the dag_worker means finished nodes are no longer removed from the DAG; since the DAG is a buffered structure, this may cause the execution to halt if the DAG is at full capacity. You can then either resize! it, or simply start the worker (which will result in the processing of the finished channel):","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"DataFlowTasks.start_dag_worker(sch)\nsch","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nThere are situations where you may want to change the scheduler temporarily to execute a block of code, and restore the default scheduler after. This can be done using the with_scheduler method. ","category":"page"},{"location":"#Limitations","page":"Getting started","title":"Limitations","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Some current limitations are listed below:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"There is no way to specify priorities for a task.\nThe main thread executes tasks, and is responsible for adding/removing nodes from the DAG. This may hinder parallelism if the main thread is given a long task since the processing of the dag will halt until the main thread becomes free again.\n...","category":"page"},{"location":"profiling/#visualization-section","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"DataFlowTasks defines two visualization tools that help when debugging and profiling parallel programs:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"a visualization of the Directed Acyclic Graph (DAG) internally representing task dependencies;\na visualization of how tasks were scheduled during a run, alongside with other information helping understand what limits the performances of the computation.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"note: Note\nVisualization tools require additional dependencies (such as Makie or GraphViz) which are only needed during the development stage. We are therefore only declaring those as weak dependencies (for Julia v1.9 and above). The user can either set up a stacked environment in which these dependencies are available, or use the DataFlowTasks.@using_opt macro which handles the environment automatically.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Let's first introduce a small example that will help illustrate the features introduced here:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"using DataFlowTasks\n\n# Utility functions\ninit!(x)    = (x .= rand())     # Write\nmutate!(x)  = (x .= exp.(x))    # Read+Write\nresult(x,y) = sum(x) + sum(y)   # Read\n\n# Main work function\nfunction work(A, B)\n    # Initialization\n    @dspawn init!(@W(A))               label=\"init A\"\n    @dspawn init!(@W(B))               label=\"init B\"\n\n    # Mutation\n    @dspawn mutate!(@RW(A))            label=\"mutate A\"\n    @dspawn mutate!(@RW(B))            label=\"mutate B\"\n\n    # Final read\n    res = @dspawn result(@R(A), @R(B)) label=\"read A,B\"\n    fetch(res)\nend","category":"page"},{"location":"profiling/#Creating-a-[LogInfo](@ref-DataFlowTasks.LogInfo)","page":"Debugging & Profiling","title":"Creating a LogInfo","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"In order to inspect code which makes use of DataFlowTasks, you can use the DataFlowTasks.@log macro to keep a trace of the various parallel events and the underlying DAG. Note that to avoid profiling the compilation time, it is often advisable to perform a \"dry run\" of the code first, as done in the example below:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"\n# Context\nA = ones(2000, 2000)\nB = ones(3000, 3000)\n\n# precompilation run\nwork(copy(A),copy(B)) \n\n# activate logging of events\nlog_info = DataFlowTasks.@log work(A, B)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The log_info object above, of LogInfo type, stores various information that can be used to reconstruct both the inferred dependencies and the parallel execution traces of the DataFlowTasks, as illustrated next.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"warning: Warning\nWhen using @log, you typically want the block of code being benchmarked to wait for the completion of its DataFlowTasks before returning (otherwise the LogInfo object that is returned may lack information regarding the DataFlowTasks that have not been completed). In the example above, that was achieved through the use of fetch in the last line of the work function.","category":"page"},{"location":"profiling/#DAG-visualization","page":"Debugging & Profiling","title":"DAG visualization","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"In order to better understand what this example does, and check that data dependencies were suitably annotated, it can be useful to look at the Directed Acyclic Graph (DAG) representing task dependencies as they were inferred by DataFlowTasks. The DAG can be visualized by creating a GraphViz.Graph out of it:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"DataFlowTasks.@using_opt GraphViz # or `using GraphViz` if your environment has it\nGraphViz.Graph(log_info)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When the working environment supports rich media, the DAG will be displayed automatically. In other cases, it is possible to export it to an image using DataFlowTasks.savedag:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"dag = GraphViz.Graph(log_info)\nDataFlowTasks.savedag(\"profiling-example.svg\", dag)\nnothing # hide","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Note how the task labels (which were provided as extra arguments to @dspawn) are used in the DAG rendering and make it more readable. In the DAG visualization, the critical path is highlighted in red: it is the sequential path that took the longest run time during the computation.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"note: Note\nThe run time of this critical path imposes a hard bound on parallel performances: no matter how many threads are available, it is not possible for the computation to take less time than the duration of the critical path.","category":"page"},{"location":"profiling/#Scheduling-and-profiling-information","page":"Debugging & Profiling","title":"Scheduling and profiling information","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The collected scheduling & profiling information can be visualized using Makie.plot on the the log_info object (note that using the GLMakie backend brings a bit more interactivity than CairoMakie):","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"DataFlowTasks.@using_opt CairoMakie # or GLMakie to benefit from more interactivity\nplot(log_info; categories=[\"init\", \"mutate\", \"read\"])\nnothing # hide","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"(Image: ProfilingExampleTrace)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The categories keyword argument allows grouping tasks in categories according to their labels. In the example above, all tasks containing \"mutate\" in their label will be grouped in the same category.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Note : be careful with giving similar labels. If tasks have \"R\" and \"RW\" labels, and the substrings given to the plot's argument are also \"R\", and \"RW\", then all tasks will be in the category \"R\" (because \"R\" can be found in \"RW\"). Regular expressions can be given instead of substrings in order to avoid such issues.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Let us explore the various parts of this graph.","category":"page"},{"location":"profiling/#Parallel-Trace","page":"Debugging & Profiling","title":"Parallel Trace","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The main plot (at the top) is the parallel trace visualization. In this example there were two threads; we can see on which thread the task was run, and the time it took.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Even though tasks are grouped in categories by considering substrings in their labels, the full label is shown when hovering over a task in the interactive visualization (i.e. when using GLMakie instead of CairoMakie).","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The plot also shows the time spent inserting nodes in the graph (which is part of the overhead incurred by the use of DataFlowTasks): these insertion times are visualized as red tasks. They are not visible for such a small example, but the interactive visualization allows zooming on the plot to search for those small tasks.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Also note that inserting tasks into the graph involves memory allocations, and may thus trigger garbage collector sweeps. When this happens, the time spent in the garbage collector is also shown in the plot.","category":"page"},{"location":"profiling/#Activity-plot","page":"Debugging & Profiling","title":"Activity plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"activity\" barplot (in the bottom left corner of the window) gives us information on the break-down of parallel computing times (summed over all threads):","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Computing represents the total time spent in the tasks bodies (i.e. \"useful\" work);\nInserting represents the total time spent inserting nodes in the DAG (i.e. overhead induced by DataFlowTasks), possibly including any time spent in the GC if it is triggered by a memory allocation in the task insertion process;\nOther represents the total idle time on all threads (which may be due to bad scheduling, or simply arise by lack of enough exposed parallelism in the algorithm).","category":"page"},{"location":"profiling/#Time-Bounds-plot","page":"Debugging & Profiling","title":"Time Bounds plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"Time Bounds\" barplot (in the bottom center of the window) tries to present insightful information about the performance limiting factors in the computation:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"critical path represents the time spent in the longest sequential path in the DAG (shown in red in the DAG visualization). As said above, it bounds the performance in that even infinitely many threads would still have to compute this path sequentially;\nwithout waiting represents the duration of a hypothetical computation in which all computing time would be evenly distributed among threads (i.e. no thread would ever have to wait). This also bounds the total time because it does not account for dependencies between tasks.\nReal represents the measured \"wall clock time\" of the computation; it should be larger than both of the aforementioned bounds.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When looking for faster response times, this graph may suggest sensible ways to explore. If the measured time is close to the critical path duration, then adding more threads will be of no help, but decomposing the work in smaller tasks may be useful. On the other hand, if the measured time is close to the \"without waiting\" bound, then adding more workers may reduce the wall clock time and scale relatively well.","category":"page"},{"location":"profiling/#Times-per-Category-plot","page":"Debugging & Profiling","title":"Times per Category plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"Times per Category\" barplot (in the bottom right of the window) displays the total time spent on all threads while performing user-defined tasks (grouped by category as explained above).","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When trying to optimize the sequential performance of the algorithm, this is where one can get data about what actually takes time (and therefore could produce large gains in performance if it could be optimized).","category":"page"},{"location":"dagger/#dagger-section","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"","category":"section"},{"location":"dagger/#What's-Dagger.jl","page":"Comparaison with Dagger.jl","title":"What's Dagger.jl","text":"","category":"section"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Dagger is a package for parallel computing, inspired by Python's Dask library, that is meant to be flexible and easy to use. It's supposed to help the parallelization of a complex serial code without the need to refactor everything. It uses a functionnal paradigm to easily imply dependencies between tasks, so they are not to be thought by the user. An exemple from Dagger.jl's documentation :  ","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"using Dagger\n\nadd1(value) = value + 1\nadd2(value) = value + 2\ncombine(a...) = sum(a)\n\np = Dagger.@spawn add1(4)\nq = Dagger.@spawn add2(p)\nr = Dagger.@spawn add1(3)\ns = Dagger.@spawn combine(p, q, r)\n\n@assert fetch(s) == 16","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"The result of the first task will be stored in p, and Dagger detects that q needs p to run, etc.. So the dependencies are automatically computed, and give the next DAG :  ","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"(Image: Dagger's DAG)  ","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Under the hood, what's happening is we don't manipulate numbers, and matrices, but EagerThunks. After the fisrt line, p has become an EagerThunk, a sort of task carrying all the informations needed by Dagger.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Because we now know the dependencies between all tasks, we can give that to a scheduler (Dagger.jl implements his own), and give those tasks to different cores.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Dagger.jl's abstraction handles multi-threading and distributed parallel computing.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Like Dask, Dagger.jl comes with it's own data structures, mainly DArrays, for distributed memory computing.","category":"page"},{"location":"dagger/#Comparison","page":"Comparaison with Dagger.jl","title":"Comparison","text":"","category":"section"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"The main points that separate working with DataFlowTasks and Dagger are :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"The approach : dependencies are not implied by variable names, but by variable's associated memory.\nData structure : data structures are not wrapped into a package's own data structure (EagerThunk).\nDistributed parallelism : not supported by DataFlowTasks. \nDagger use a functionnal paradigm.\nScheduler : Dagger has it's own scheduler, where DataFlowTasks uses Julia's default one.\nPerformances (see below)","category":"page"},{"location":"dagger/#Case-study","page":"Comparaison with Dagger.jl","title":"Case study","text":"","category":"section"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"DataFlowTasks is oriented towards linear algebra matrix computations, let's see how it can be prefered as Dagger.jl in that case by looking at the cholesky tiled factorization algorithm. We'll consider our matrix A already divided in blocks, where Aij is a view of the block at index (i,j).   The pseudo-code for this algorithm would be :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Requires : A of size m*n \nfor i in 1:m\n    Aii <- cholesky(Aii)\n    for j in i+1:m\n        Aij <- ldiv(Aii,Aij)\n    end\n    for j in i+1:m\n        for k in j:n\n            Ajk <- schurcomplement(Ajk, Aji, Aik)\n        end\n    end\nend","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"In the first place, we can see that Dagger.jl's functionnal paradigm behaves like what we are used to write in pseudo-code : Aii <- cholesky(Aii). Usually though, code would written like : cholesky!(Aii), the function modifying the variable.  ","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"The problem here is that in this code, we'll only use a couple of variables names : Aii, Aij, Ajk etc... that will represent, depending on the iteration, a different matrix block.   To illustrate :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"p = Dagger.@spawn add1(4)\np = Dagger.@spawn add2(2)\nq = Dagger.@spawn add1(p)","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Here the first task is shadowed by second, q will only wait for the second task.   Therefore in the cholesky tiled factorization, we have to have a single variable name for every block of memory. Before computing anything we have to change our paradigm : we can't manipulate blocks of memory, we have to manipulate Eagerthunks previously mapped to blocks of memory.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"# Map thunks to blocks of memory\nthunks = Matrix{Dagger.EagerThunk}(undef, m, n)\n# ...\n\n# Work on thunks\nfor i in 1:m\n    thunks[i, i] = Dagger.@spawn cholesky(thunks[i, i])\n    # ...\nend\n\n# Reverse mapping from thunks to blocks of memory\nfor i in 1:m, j in i:n\n    Aij .= fetch(thunks[i, j])\nend","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"It can be more natural to reason on memory access, rather than on return values stored by variables. The DataFlowTasks cholesky tiled factorization would look more similar to the common pseudo-code showed above :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"for i in 1:m\n    @dpsawn cholesky!(@RW(Aii))\n    for j in i+1:m\n        @dspawn ldiv!(@R(L), @RW(Aij))\n    end\n    for j in i+1:m\n        for k in j:n\n            @dspawn matmul!(@RW(Ajk), @R(Aji), @R(Aik))\n        end\n    end\nend","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"With DataFlowTasks, the approach is thinking in an isolated way, at the moment of writing the function call, what are the modes of access of the variables. There's no need to take the whole code into account.","category":"page"},{"location":"dagger/#Write-After-Read","page":"Comparaison with Dagger.jl","title":"Write After Read","text":"","category":"section"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Dagger.jl doesn't detect this kind of dependcies (WAR). Although it's not the most common type of depency, it's still worth noticing. Let's look at a simple example.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Let a vector of 4 elements X = ones(4), with 2 views X₁ = @views X[1:2] and X₂ = @views X[3:4]. We reproduce here the behaviour of the data structures we used in the cholesky tiled factorization exemple. We will use the 2 next functions to work on this data structure.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"function longTask(Xᵢ...)\n    sleep(2)\n    Xᵢ[1] .*= (2.0 .+ Xᵢ[2])\nend\nfunction shortTask(Xᵢ...)\n    Xᵢ[1] .+= 1.0\nend","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"The work we want to do will be of type : RW(X₁) -> RW(X₂) R(X₁) -> RW(X₁), and we will name those 3 tasks tᵢ with i ∈ [1, 2, 3]. The code will be :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"X₁ = Dagger.@spawn shortTask(X₁)\nX₂ = Dagger.@spawn longTask(X₂, X₁)\n# fetch(X₂) needs to be added if we want it to work\nX₁ = Dagger.@spawn shortTask(X₁)\n\nfetch(X₁)\nfetch(X₂)\nX","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"We could think that because X₁ is in argument in t₂, when we'll want to write on X₁ in t₃, we will wait for t₂ to be finished. If it's the case, will have the following stats for X (the middle bar represent the separation X₁ and X₂ induce) :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"1 1 | 1 1\n2 2 | 1 1  --> t₁\n1 1 | 4 4  --> t₂\n3 3 | 4 4  --> t₃","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Instead if we don't wait for t₂, we'll have an inversion of t₃ and t₂. We will have :","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"1 1 | 1 1\n2 2 | 1 1  --> t₁\n3 3 | 1 1  --> t₃\n3 3 | 5 5  --> t₂","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"Actually, it's the case when the tasks are meant to be of different times like they are now (to illustrate the point). If they are not so different with each other, the code becomes non-determinstic.","category":"page"},{"location":"dagger/","page":"Comparaison with Dagger.jl","title":"Comparaison with Dagger.jl","text":"!!! TO DO : PERFORMANCE DIFFERENCES !!!","category":"page"}]
}
