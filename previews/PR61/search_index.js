var documenterSearchIndex = {"docs":
[{"location":"references/#references-section","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Modules = [DataFlowTasks, GraphViz_Ext, Makie_Ext]","category":"page"},{"location":"references/#DataFlowTasks.DataFlowTasks","page":"References","title":"DataFlowTasks.DataFlowTasks","text":"module DataFlowTask\n\nCreate Tasks which can keep track of how data flows through it.\n\n\n\n\n\n","category":"module"},{"location":"references/#DataFlowTasks.LOGINFO","page":"References","title":"DataFlowTasks.LOGINFO","text":"const LOGINFO::Ref{LogInfo}\n\nGlobal LogInfo being used to record the events. Can be changed using _setloginfo!.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.TASKCOUNTER","page":"References","title":"DataFlowTasks.TASKCOUNTER","text":"const TASKCOUNTER::Threads.Atomic{Int64}\n\nGlobal counter of created DataFlowTasks.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.TASKGRAPH","page":"References","title":"DataFlowTasks.TASKGRAPH","text":"const TASKGRAPH::Ref{TASKGRAPH}\n\nThe active TaskGraph being used. Nodes will be added to this TaskGraph by default.\n\nCan be changed using set_active_taskgraph!.\n\n\n\n\n\n","category":"constant"},{"location":"references/#DataFlowTasks.AccessMode","page":"References","title":"DataFlowTasks.AccessMode","text":"@enum AccessMode READ WRITE READWRITE\n\nDescribe how a DataFlowTask access its data.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.DAG","page":"References","title":"DataFlowTasks.DAG","text":"struct DAG{T}\n\nRepresentation of a directed acyclic graph containing nodes of type T. The list of nodes with edges coming into a node i can be retrieved using inneighbors(dag,i); similarly, the list of nodes with edges leaving from i can be retrieved using outneighbors(dag,i).\n\nDAG is a buffered structure with a buffer of size sz_max: calling addnode! on it will block if the DAG has more than sz_max elements.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.DAG-Union{Tuple{}, Tuple{Any}, Tuple{T}} where T","page":"References","title":"DataFlowTasks.DAG","text":"DAG{T}(sz)\n\nCreate a buffered DAG holding a maximum of sz nodes of type T.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.DataFlowTask","page":"References","title":"DataFlowTasks.DataFlowTask","text":"DataFlowTask(func,data,mode)\n\nCreate a task-like object similar to Task(func) which accesses data with AccessMode mode.\n\nWhen a DataFlowTask is created, the elements in its data field will be checked against all other active DataFlowTask to determined if a dependency is present based on a data-flow analysis. The resulting Task will then wait on those dependencies.\n\nA DataFlowTask behaves much like a Julia Task: you can call wait(t), schedule(t) and fetch(t) on it.\n\nSee also: @dtask, @spawn, @dasync.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.FinishedChannel","page":"References","title":"DataFlowTasks.FinishedChannel","text":"struct FinishedChannel{T} <: AbstractChannel{T}\n\nUsed to store tasks which have been completed, but not yet removed from the underlying DAG. Taking from an empty FinishedChannel will block.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.InsertionLog","page":"References","title":"DataFlowTasks.InsertionLog","text":"struct InsertionLog\n\nLogs the execution trace of a DataFlowTask insertion.\n\nFields:\n\ntime_start  : time the insertion began\ntime_finish : time the insertion finished\ntaskid      : the task it is inserting\ntid         : the thread on which the insertion is happening\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.LogInfo","page":"References","title":"DataFlowTasks.LogInfo","text":"struct LogInfo\n\nContains informations on the program's progress. For thread-safety, the LogInfo structure uses one vector of TaskLog per thread.\n\nYou can visualize and postprocess a LogInfo using GraphViz.Graph and Makie.plot.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.Stop","page":"References","title":"DataFlowTasks.Stop","text":"struct Stop\n\nSingleton type used to safely interrupt a task reading from an AbstractChannel.\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.TaskGraph","page":"References","title":"DataFlowTasks.TaskGraph","text":"struct TaskGraph\n\nA directed acyclic graph used to reprenset the dependencies between DataFlowTasks.\n\nTaskGraph(sz) creates a task graph that can hold at most sz elements at any given time. In particular, trying to add a new DataFlowTask will block if the TaskGraph is already full.\n\nSee also: get_active_taskgraph, set_active_taskgraph!\n\n\n\n\n\n","category":"type"},{"location":"references/#DataFlowTasks.TaskLog","page":"References","title":"DataFlowTasks.TaskLog","text":"struct TaskLog\n\nLogs the execution trace of a DataFlowTask.\n\nFields:\n\ntag         : task id in DAG\ntime_start  : time the task started running\ntime_finish : time the task finished running\ntid         : thread on which the task ran\ninneighbors : vector of incoming neighbors in DAG\nlabel       : a string used for displaying and/or postprocessing tasks\n\n\n\n\n\n","category":"type"},{"location":"references/#Base.empty!-Tuple{DataFlowTasks.TaskGraph}","page":"References","title":"Base.empty!","text":"empty!(tg::TaskGraph)\n\nInterrupt all tasks in tg and remove them from the underlying DAG.\n\nThis function is useful to avoid having to restart the REPL when a task in tg errors.\n\n\n\n\n\n","category":"method"},{"location":"references/#Base.resize!-Tuple{DataFlowTasks.TaskGraph, Any}","page":"References","title":"Base.resize!","text":"resize!(tg::TaskGraph, sz)\n\nChange the buffer size of tg to sz.\n\n\n\n\n\n","category":"method"},{"location":"references/#Base.wait-Tuple{DataFlowTasks.TaskGraph}","page":"References","title":"Base.wait","text":"wait(tg::TaskGraph)\n\nWait for all nodes in tg to be finished before continuining.\n\nTo wait on the active TaskGraph, use wait(get_active_taskgraph()).\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks._getloginfo-Tuple{}","page":"References","title":"DataFlowTasks._getloginfo","text":"_getloginfo()\n\nReturn the active logger.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks._setloginfo!-Tuple{Union{Nothing, DataFlowTasks.LogInfo}}","page":"References","title":"DataFlowTasks._setloginfo!","text":"_setloginfo!(l::LogInfo)\n\nSet the active logger to l.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.access_mode-Tuple{DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.access_mode","text":"access_mode(t::DataFlowTask[,i])\n\nHow t accesses its data.\n\nSee: AccessMode\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addedge!-Union{Tuple{T}, Tuple{DataFlowTasks.DAG{T}, T, T}} where T","page":"References","title":"DataFlowTasks.addedge!","text":"addedge!(dag,i,j)\n\nAdd (directed) edge connecting node i to node j in the dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addedge_transitive!-Tuple{Any, Any, Any}","page":"References","title":"DataFlowTasks.addedge_transitive!","text":"addedge_transitive!(dag,i,j)\n\nAdd edge connecting nodes i and j if there is no path connecting them already.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.addnode!-Union{Tuple{T}, Tuple{DataFlowTasks.DAG{T}, T}, Tuple{DataFlowTasks.DAG{T}, T, Any}} where T","page":"References","title":"DataFlowTasks.addnode!","text":"addnode!(dag,(k,v)::Pair[, check=false])\naddnode!(dag,k[, check=false])\n\nAdd a node to the dag. If passed only a key k, the value v is initialized as empty (no edges added). The check flag is used to indicate if a data flow analysis should be performed to update the dependencies of the newly inserted node.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.capacity-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.capacity","text":"capacity(dag)\n\nThe maximum number of nodes that dag can contain.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.data-Tuple{DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.data","text":"data(t::DataFlowTask[,i])\n\nData accessed by t.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.data_dependency-Tuple{DataFlowTasks.DataFlowTask, DataFlowTasks.DataFlowTask}","page":"References","title":"DataFlowTasks.data_dependency","text":"data_dependency(t1::DataFlowTask,t1::DataFlowTask)\n\nDetermines if there is a data dependency between t1 and t2 based on the data they read from and write to.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.enable_debug","page":"References","title":"DataFlowTasks.enable_debug","text":"enable_debug(mode = true)\n\nIf mode is true (the default), enable debug mode: errors inside tasks will be shown.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.enable_log","page":"References","title":"DataFlowTasks.enable_log","text":"enable_log(mode = true)\n\nIf mode is true (the default), logging is enabled throug the @log macro. Calling enable_log(false) will de-activate logging at compile time to avoid any possible overhead.\n\nNote that changing the log mode at runtime will may invalidate code, possibly triggering recompilation.\n\nSee also: @log, with_logging\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.force_linear_dag","page":"References","title":"DataFlowTasks.force_linear_dag","text":"force_linear_dag(mode=false)\n\nIf mode is true, nodes are added to the DAG in a linear fashion, i.e. the DAG connects node i to node i+1. This is useful for debugging purposes.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.force_sequential","page":"References","title":"DataFlowTasks.force_sequential","text":"force_sequential(mode = true)\n\nIf mode is true, enable sequential mode: no tasks are created and scheduled, code is simply run as it appears in the sources. In effect, this makes @spawn a no-op.\n\nBy default, sequential mode is disabled when the program starts.\n\nSee also: force_linear_dag.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.get_active_taskgraph-Tuple{}","page":"References","title":"DataFlowTasks.get_active_taskgraph","text":"get_active_taskgraph()\n\nReturn the active TaskGraph.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.has_edge-Tuple{DataFlowTasks.DAG, Any, Any}","page":"References","title":"DataFlowTasks.has_edge","text":"has_edge(dag,i,j)\n\nCheck if there is an edge connecting i to j.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.inneighbors-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.inneighbors","text":"inneighbors(dag,i)\n\nList of predecessors of i in dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.isconnected-Tuple{DataFlowTasks.DAG, Any, Any}","page":"References","title":"DataFlowTasks.isconnected","text":"isconnected(dag,i,j)\n\nCheck if there is a path in dag connecting i to j.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.memory_overlap-Tuple{Any, Any}","page":"References","title":"DataFlowTasks.memory_overlap","text":"memory_overlap(di,dj)\n\nDetermine if data di and dj have overlapping memory in the sense that mutating di can change dj (or vice versa). This function is used to build the dependency graph between DataFlowTasks.\n\nA generic version is implemented returning true (but printing a warning). Users should overload this function for the specific data types used in the arguments to allow for appropriate inference of data dependencies.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.memory_overlap-Tuple{Array, Array}","page":"References","title":"DataFlowTasks.memory_overlap","text":"memory_overlap(di::AbstractArray,dj::AbstractArray)\n\nTry to determine if the arrays di and dj have overlapping memory.\n\nWhen both di and dj are Arrays of bitstype, simply compare their addresses. Otherwise, compare their parents by default.\n\nWhen both di and dj are SubArrays we compare the actual indices of the SubArrays when their parents are the same (to avoid too many false positives).\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.nodes-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.nodes","text":"nodes(dag::DAG)\n\nReturn an iterator over the nodes of dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.num_edges-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.num_edges","text":"num_edges(dag::DAG)\n\nNumber of edges in the DAG.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.num_nodes-Tuple{DataFlowTasks.DAG}","page":"References","title":"DataFlowTasks.num_nodes","text":"num_nodes(dag::DAG)\n\nNumber of nodes in the DAG.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.outneighbors-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.outneighbors","text":"outneighbors(dag,i)\n\nList of successors of j in dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.remove_node!-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.remove_node!","text":"remove_node!(dag::DAG,i)\n\nRemove node i and all of its edges from dag.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.savedag","page":"References","title":"DataFlowTasks.savedag","text":"DataFlowTasks.savedag(filepath, graph)\n\nSave graph as an SVG image at filepath. This requires GraphViz to be available.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.set_active_taskgraph!-Tuple{Any}","page":"References","title":"DataFlowTasks.set_active_taskgraph!","text":"set_active_taskgraph!(tg)\n\nSet the active TaskGraph to tg.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.stack_weakdeps_env!-Tuple{}","page":"References","title":"DataFlowTasks.stack_weakdeps_env!","text":"DataFlowTasks.stack_weakdeps_env!(; verbose = false, update = false)\n\nPush to the load stack an environment providing the weak dependencies of DataFlowTasks. During the development stage, this allows benefiting from the profiling / debugging features of DataFlowTasks without having to install GraphViz or Makie in the project environment.\n\nThis can take quite some time if packages have to be installed or precompiled. Run in verbose mode to see what happens.\n\nAdditionally, set update=true if you want to update the weakdeps environment.\n\nwarning: Warning\nThis feature is experimental and might break in the future.\n\nExamples:\n\nDataFlowTasks.stack_weakdeps_env!()\nusing GraphViz\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.start_dag_cleaner","page":"References","title":"DataFlowTasks.start_dag_cleaner","text":"start_dag_cleaner(tg)\n\nStart a task associated with tg which takes nodes from its finished queue and removes them from the dag. The task blocks if finished is empty.\n\n\n\n\n\n","category":"function"},{"location":"references/#DataFlowTasks.update_edges!-Tuple{DataFlowTasks.DAG, Any}","page":"References","title":"DataFlowTasks.update_edges!","text":"update_edges!(dag::DAG,i)\n\nPerform the data-flow analysis to update the edges of node i. Both incoming and outgoing edges are updated.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_logging!-Tuple{Any, DataFlowTasks.LogInfo}","page":"References","title":"DataFlowTasks.with_logging!","text":"with_logging!(f,l::LogInfo)\n\nSimilar to with_logging, but append events to l.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_logging-Tuple{Any}","page":"References","title":"DataFlowTasks.with_logging","text":"with_logging(f) --> f(),loginfo\n\nExecute f() and log DataFlowTasks into the loginfo object.\n\nExamples:\n\nusing DataFlowTasks: @spawn\n\nA,B = zeros(2), ones(2);\n\nout,loginfo = DataFlowTasks.with_logging() do\n    @spawn fill!(@W(A),1)\n    @spawn fill!(@W(B),1)\n    res = @spawn sum(@R(A)) + sum(@R(B))\n    fetch(res)\nend\n\n#\n\nout\n\nSee also: LogInfo\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.with_taskgraph-Tuple{Any, Any}","page":"References","title":"DataFlowTasks.with_taskgraph","text":"with_taskgraph(f,tg::TaskGraph)\n\nRun f, but push DataFlowTasks to tg.\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks.@dasync-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@dasync","text":"@dasync expr [kwargs...]\n\nLike @spawn, but schedules the task to run on the current thread.\n\nSee also:\n\n@spawn, @dtask\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@dtask-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@dtask","text":"@dtask expr [kwargs...]\n\nCreate a DataFlowTask to execute expr, where data have been tagged to specify how they are accessed. Note that the task is not automatically scheduled for execution.\n\nSee @spawn for information on how to annotate expr to specify data dependencies, and a list of supported keyword arguments.\n\nSee also: @spawn, @dasync\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@log-Tuple{Any}","page":"References","title":"DataFlowTasks.@log","text":"DataFlowTasks.@log expr --> LogInfo\n\nExecute expr and return a LogInfo instance with the recorded events. The Logger waits for the current taskgraph (see get_active_taskgraph to be empty before starting.\n\nwarning: Warning\nThe returned LogInfo instance may be incomplete if block returns before all DataFlowTasks spawened inside of it are completed. Typically expr should fetch the outcome before returning to properly benchmark the code that it runs (and not merely the tasks that it spawns).\n\nSee also: with_logging, with_logging!\n\n\n\n\n\n","category":"macro"},{"location":"references/#DataFlowTasks.@spawn-Tuple{Any, Vararg{Any}}","page":"References","title":"DataFlowTasks.@spawn","text":"@spawn expr [kwargs...]\n\nSpawn a Julia Task to execute the code given by expr, and schedule it to run on any available thread.\n\nAnnotate the code in expr with @R, @W and/or @RW to indicate how it accesses data (see examples below). This information is used to automatically infer task dependencies.\n\nAdditionally, the following keyword arguments can be provided:\n\nlabel: provide a label to identify the task. This is useful when logging scheduling information;\npriority: inform the scheduler about the relative priority of the task. This information is not (yet) leveraged by the default scheduler.\n\nExamples:\n\nBelow are 3 equivalent ways to create the same Task, which expresses a Read-Write dependency on C and Read dependencies on A and B\n\nusing LinearAlgebra\nusing DataFlowTasks: @spawn\nA = ones(5, 5)\nB = ones(5, 5)\nC = zeros(5, 5)\nα, β = (1, 0)\n\n# Option 1: annotate arguments in a function call\n@spawn mul!(@RW(C), @R(A), @R(B), α, β)\n\n# Option 2: specify data access modes in the code block\n@spawn begin\n   @RW C\n   @R  A B\n   mul!(C, A, B, α, β)\nend\n\n# Option 3: specify data access modes after the code block\n# (i.e. alongside keyword arguments)\nres = @spawn mul!(C, A, B, α, β) @RW(C) @R(A,B)\n\nfetch(res) # a 5×5 matrix of 5.0\n\nHere is a more complete example, demonstrating a full computation involving 2 different tasks.\n\nusing DataFlowTasks: @spawn\n\nA = rand(5)\n\n# create a task with WRITE access mode to A\n# and label \"writer\"\nt1 = @spawn begin\n    @W A\n    sleep(1)\n    fill!(A,0)\n    println(\"finished writing\")\nend  label=\"writer\"\n\n# create a task with READ access mode to A\nt2 = @spawn begin\n    @R A\n    println(\"I automatically wait for `t1` to finish\")\n    sum(A)\nend  priority=1\n\nfetch(t2) # 0\n\n# output\n\nfinished writing\nI automatically wait for `t1` to finish\n0.0\n\nNote that in the example above t2 waited for t1 because it read a data field that t1 accessed in a writable manner.\n\n\n\n\n\n","category":"macro"},{"location":"references/#GraphViz.Graph-Tuple{DataFlowTasks.LogInfo}","page":"References","title":"GraphViz.Graph","text":"GraphViz.Graph(log_info::LogInfo)\n\nProduce a GraphViz.Graph representing the DAG of tasks collected in log_info.\n\nSee also: DataFlowTasks.@log\n\n\n\n\n\n","category":"method"},{"location":"references/#DataFlowTasks_GraphViz_Ext.loggertodot-Tuple{Any}","page":"References","title":"DataFlowTasks_GraphViz_Ext.loggertodot","text":"loggertodot(logger)  --> dagstring\n\nReturn a string in the DOT format representing the underlying graph in logger and to be plotted by GraphViz with Graph(loggertodot())\n\n\n\n\n\n","category":"method"},{"location":"references/#MakieCore.plot-Tuple{DataFlowTasks.LogInfo}","page":"References","title":"MakieCore.plot","text":"plot(log_info; categories)\n\nPlot DataFlowTasks log_info labeled informations with categories.\n\nEntries in categories define how to group tasks in categories for plotting. Each entry can be:\n\na String: in this case, all tasks having labels in which the string occurs are grouped together. The string is also used as a label for the category itself.\na String => Regex pair: in this case, all tasks having labels matching the regex are grouped together. The string is used as a label for the category itself.\n\nSee the documentation for more information on how to profile and visualize DataFlowTasks.\n\n\n\n\n\n","category":"method"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"EditURL = \"blur-roberts.jl\"","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Blur-and-Roberts-image-filters","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"(Image: ipynb) (Image: nbviewer)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"This example illustrate the use of DataFlowTasks.jl to parallelize the tiled application of two kernels used in image processing. The application first applies a blur filter on each pixel of the image; in a second step, the Roberts cross operator is applied to detect edges in the image.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Let us first load a test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using Images\nurl = \"https://upload.wikimedia.org/wikipedia/commons/c/c3/Equus_zebra_hartmannae_-_Etosha_2015.jpg\"\nispath(\"test-image.jpg\") || download(url, \"test-image.jpg\")\nimg = Gray.(load(\"test-image.jpg\"))","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"We start by defining a few helper functions:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"the contract and expand functions manipulate ranges of indices in order to respectively contract or expand them by a few pixels;\nthe img2mat and mat2img convert between a Gray-scale image and a matrix of floating-point pixel intensities. The filters will work on this latter representation, which may need a renormalization to be converted back to a Gray-scale image.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"contract(range,n) = range[begin+n:end-n]\nexpand(range,n)   = range[begin]-n:range[end]-n\n\nfunction img2mat(img)\n    PixelType = eltype(img)\n    mat = Float64.(img)\n    return (PixelType, mat)\nend\n\nfunction mat2img(PixelType, mat)\n    m1, m2 = extrema(mat)\n    PixelType.((mat .- m1) ./ (m2-m1))\nend\n\nPixelType, mat = img2mat(img);\nnothing #hide","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Filters-implementation","page":"Blur & Roberts image filters","title":"Filters implementation","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The blur! function averages the value of each pixel with the values of all pixels less than width pixels away in manhattan distance. In order to simplify the implementation, the filter is applied only to pixels that are sufficiently far from the boundary to have all their neighbors correctly defined.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Results are written in-place in a pre-allocated dest array. Unless otherwise specified, the filter is applied to the whole image, but can be reduced to a tile if a smaller range argument is provided.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur!(dest, src; range=axes(src), width)\n    ri, rj = intersect.(range, contract.(axes(src), width))\n\n    weight = 1/(2*width+1)^2\n    @inbounds for i in ri, j in rj\n        dest[i,j] = 0\n        for δi in -width:width, δj in -width:width\n            dest[i,j] += src[i+δi, j+δi]\n        end\n        dest[i,j] *= weight\n    end\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"In the following, we'll use a filter width of 5 pixels, which produces the following results on the test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"width = 5\nblurred = similar(mat)\n\nblur!(blurred, mat; width)\n\nmat2img(PixelType, blurred)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The roberts! function applies the Roberts cross operator to the provided image. Like above, it operates by default on all pixels in the image (provided they are sufficiently far from the boundaries), but can be restricted to work on a tile if the range argument is provided.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function roberts!(dest, src; range=axes(src))\n    ri, rj = intersect.(range, contract.(axes(src), 1))\n\n    for i in ri, j in rj\n        dest[i,j] = (\n            + (sqrt(src[i,  j]) - sqrt(src[i+1,j+1]))^2\n            + (sqrt(src[i+1,j]) - sqrt(src[i  ,j+1]))^2\n        )^(0.25)\n    end\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Applying this edge detection filter on the original image produces the following results:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"contour = similar(mat)\nroberts!(contour, mat)\n\nmat2img(PixelType, contour)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Chaining the blur and roberts filters may make edge detection less noisy:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur_roberts!(img; width, tmp=similar(img))\n    blur!(tmp, img; width)\n    roberts!(img, tmp)\nend\n\nmat1 = copy(mat)\ntmp  = similar(mat)\n\nblur_roberts!(mat1; width, tmp)\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Tiled-filter-application","page":"Blur & Roberts image filters","title":"Tiled filter application","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The TiledIteration.jl package implements various tools allowing to define and iterate over disjoint tiles of a larger array. We'll use it to apply the filters tile by tile.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The map_tiled! higher-order function automates the application of a filter fun! on all pixels of an image src decomposed with a tilesize ts. This higher-order function is then used to defined tiled versions of the blur and roberts filters.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using TiledIteration\n\nfunction map_tiled!(fun!, dest, src, ts)\n    for tile in TileIterator(axes(src), (ts, ts))\n        fun!(dest, src, tile)\n    end\nend\n\nblur_tiled!(dest, src, ts; width) = map_tiled!(dest, src, ts) do dest, src, tile\n    blur!(dest, src; width, range=tile)\nend\n\nroberts_tiled!(dest, src, ts) = map_tiled!(dest, src, ts) do dest, src, tile\n    roberts!(dest, src; range=tile)\nend\n\nfunction blur_roberts_tiled!(img, ts; width, tmp=similar(img))\n    blur_tiled!(tmp, img, ts; width)\n    roberts_tiled!(img, tmp, ts)\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Decomposing the original image in tiles of size 512times 512, the tiled application of the filters yields the same result as above, in a more cache-efficient way:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"ts = 512\n\nmat1 .= mat\nblur_roberts_tiled!(mat1, ts; width, tmp)\n\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Parallel-filter-application","page":"Blur & Roberts image filters","title":"Parallel filter application","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Parallelizing the tiled filter application is relatively straightforward using DataFlowTasks.jl. As usual, it involves specifying which data is accessed by each task.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"using DataFlowTasks\nusing DataFlowTasks: @spawn\n\nfunction blur_dft!(dest, src, ts; width)\n    map_tiled!(dest, src, ts) do dest, src, tile\n        outer = intersect.(expand.(tile, width), axes(src))\n        @spawn begin\n            @R view(src, outer...)\n            @W view(dest, tile...)\n            blur!(dest, src; width, range=tile)\n        end label=\"blur ($tile)\"\n    end\n    @spawn @R(dest) label=\"blur (result)\"\nend\n\nfunction roberts_dft!(dest, src, ts)\n    map_tiled!(dest, src, ts) do dest, src, tile\n        outer = intersect.(expand.(tile, 1), axes(src))\n        @spawn begin\n            @R view(src, outer...)\n            @W view(dest, tile...)\n            roberts!(dest, src; range=tile)\n        end label=\"roberts ($tile)\"\n    end\n    @spawn @R(dest) label=\"roberts (result)\"\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Note how each filter spawns one task for each tile, and an extra task to get the results in the end. This allows applying a given filter independently of the other.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"However, the filters remain composable: when applying both filters one after the other, no implicit synchronization is enforced at the end of the blurring stage, and the runtime may decide to intersperse blurring and roberts tasks (as long as the blurring of a tile and all its neighbors is performed before the application of the roberts filter on this tile).","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"function blur_roberts_dft!(img, ts; width, tmp=similar(img))\n    blur_dft!(tmp, img, ts; width)\n    roberts_dft!(img, tmp, ts)\n    @spawn @R(img) label=\"result\"\nend","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"Again this yields the same results on the test image:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"mat1 .= mat;\nblur_roberts_dft!(mat1, ts; width, tmp) |> wait\n\nmat2img(PixelType, mat1)","category":"page"},{"location":"examples/blur-roberts/blur-roberts/#Profiling-the-parallel-version","page":"Blur & Roberts image filters","title":"Profiling the parallel version","text":"","category":"section"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"As usual, profiling data should be collected in a context that is as clean as possible.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"GC.gc()\n\nmat1 .= mat;\nlog_info = DataFlowTasks.@log wait(blur_roberts_dft!(mat1, ts; width, tmp))","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"The parallel trace shows how blur and roberts tasks are interspersed in the time line:","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"DataFlowTasks.stack_weakdeps_env!()\nusing CairoMakie\n\ntrace = plot(log_info, categories=[\"blur\", \"roberts\"])","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"In terms of performance, elapsed time seems to be bounded in this case by the total computing time of all threads. Re-running the same computation with more threads may help reduce the overall wall-clock time.","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"","category":"page"},{"location":"examples/blur-roberts/blur-roberts/","page":"Blur & Roberts image filters","title":"Blur & Roberts image filters","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"CurrentModule = DataFlowTasks","category":"page"},{"location":"#DataFlowTasks","page":"Getting started","title":"DataFlowTasks","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Tasks which automatically respect data-flow dependencies","category":"page"},{"location":"#Basic-usage","page":"Getting started","title":"Basic usage","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"This package defines a @spawn macro type which behaves very much like Threads.@spawn, except that it allows the user to specify explicit data dependencies for the spawned Task. This information is then be used to automatically infer task dependencies by constructing and analyzing a directed acyclic graph based on how tasks access the underlying data. The premise is that it is sometimes simpler to specify how tasks depend on data than to specify how tasks depend on each other.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"When creating a Task using DataFlowTasks.@spawn, the following annotations can be used to declare how the Task accesses the data:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"read-only: @R or @READ\nwrite-only: @W or @WRITE\nread-write: @RW or @READWRITE","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"An @R(A) annotation for example implies that data A will be accessed in read-only mode by the task. Here is a simple example:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: @spawn\n\nn = 100_000\nA = ones(n)\n\nd1 = @spawn begin\n    @RW A\n\n    # in-place work on A\n    for i in eachindex(A)\n        A[i] = log(A[i]) # A[i] = 0\n    end\nend\n\n# reduce A\nd2 = @spawn sum(@R A)\n# The above is a shortcut for:\n#   d2 = @spawn begin\n#       @R A\n#       sum(A)\n#   end\n\nc = fetch(d2) # 0","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Two (asynchronous) tasks were created, both of which access the array A. Because d1 writes to A, and d2 reads from it, the outcome C is nondeterministic unless we specify an order of precedence. DataFlowTasks reinforces the sequential consistency criterion, which is to say that executing tasks in parallel must preserve, up to rounding errors, the result that would have been obtained if they were executed sequentially, following the order in which they were created. In the example above, this means d2 will wait on d1 because of an inferred data dependency. The outcome is thus always zero.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nIf you replace DataFlowTasks.@spawn by Threads.@spawn in the example above (and pick an n large enough) you will see that you no longer get 0 because d2 may access an element of A before it has been replaced by zero!","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nIn the d2 example above, a shortcut syntax was introduced, which allows putting access mode annotations directly around arguments in a function call. This is especially useful when the task body is a one-liner. See @spawn for an exhaustive list of supported ways to create tasks and specify data dependencies.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"No parallelism was allowed in the previous example due to a data conflict. To see that when parallelism is possible, DataFlowTasks will exploit it, consider this one last example:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: @spawn\n\nfunction run(A)\n    d1 = @spawn begin\n        @W A # write to A\n        sleep(1)\n        fill!(A,0)\n    end\n\n    # a reduction on A\n    d2 = @spawn begin\n        @R A # read from A\n        sleep(10)\n        sum(A)\n    end\n\n    # another reduction on A\n    d3 = @spawn sum(@R(A))\n\n    t = @elapsed c = fetch(d3)\n\n    @show t,c\nend\n\nA = ones(10)\nrun(A)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We see that the elapsed time to fetch the result from d3 is on the order of one second. This is expected since d3 needs to wait on d1 but can be executed concurrently with d2. The result is, as expected, 0.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"All examples this far have been simple enough that the dependencies between the tasks could (and probably should) have been inserted by hand. There are certain problems, however, where the constant reuse of memory (mostly for performance reasons) makes a data-flow approach to parallelism a rather natural way to implicitly describe task dependencies. This is the case, for instance, of tiled (also called blocked) matrix factorization algorithms, where task dependencies can become rather difficult to describe in an explicit manner. The tiled factorization section showcases some non-trivial problems for which DataFlowTasks may be useful.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nThe main goal of DataFlowTasks is to expose parallelism: two tasks ti and tj can be executed concurrently if one does not write to memory that the other reads. This data-dependency check is done dynamically, and therefore is not limited to tasks in the same lexical scope. Of course, there is an overhead associated with these checks, so whether performance gains can be obtained depend largely on how parallel the algorithm is, as well as how long each individual task takes (compared to the overhead).","category":"page"},{"location":"#Custom-types","page":"Getting started","title":"Custom types","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"In order to infer dependencies between DataFlowTasks, we must be able to determine whether two objects A and B share a common memory space. That is to say, we must know if mutating A can affect B, or vice-versa. This check is performed by the memory_overlap function:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: memory_overlap\n\nA = rand(10,10)\nB = view(A,1:10)\nC = view(A,11:20)\n\nmemory_overlap(A,B), memory_overlap(A,C), memory_overlap(B,C)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The example above works because memory_overlap has been defined for some basic AbstractArrays types inside DataFlowTasks. If a specialized method for memory_overlap is not found, DataFlowTasks errs on the safe side and falls back to a generic implementation that always returns true:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: memory_overlap\n\nstruct CirculantMatrix # a custom type\n    data::Vector{Float64}\nend\n\nv = rand(10);\nM = CirculantMatrix(v);\n\nmemory_overlap(M,copy(v))","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The warning message printed above hints at what should be done:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"import DataFlowTasks: memory_overlap\nmemory_overlap(M::CirculantMatrix,v) = memory_overlap(M.data,v) # overload\nmemory_overlap(v,M::CirculantMatrix) = memory_overlap(M,v)\nmemory_overlap(M,v), memory_overlap(M,copy(v))","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"You can now spawn tasks with your custom type CirculantMatrix as a data dependency, and things should work as expected:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks: @spawn\n\nv  = ones(5);\nM1 = CirculantMatrix(v);\nM2 = CirculantMatrix(copy(v));\n\nBase.sum(M::CirculantMatrix) = length(M.data)*sum(M.data)\n\nd1 = @spawn begin\n    @W v\n    sleep(0.5)\n    fill!(v,0) \nend;\nd2 = @spawn sum(@R M1)\nd3 = @spawn sum(@R M2)\n\nfetch(d3) # 25\nfetch(d2) # 0\n\nnothing # hide","category":"page"},{"location":"#Task-graph","page":"Getting started","title":"Task graph","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Each time a Task is spawned using DataFlowTasks.@spawn, it is added to an internal TaskGraph (see get_active_taskgraph) so that its data-dependencies can be tracked and analyzed. There are two important things to know about TaskGraph objects. First, they are buffered to handle at most sz_max tasks at a time: trying to add a task to the TaskGraph when it is full will block. This is done to keep the cost of analyzing the data dependencies under control. You can modify the buffer size as follows:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using DataFlowTasks # hide\ntaskgraph = DataFlowTasks.get_active_taskgraph()\nresize!(taskgraph,200)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Second, when the computation of a task in the TaskGraph is completed, it gets pushed into a finished channel, to be eventually processed and poped from the graph by the dag_cleaner. This is done to avoid concurrent access to the DAG: only the dag_cleaner should modify it. If you want to stop nodes from being removed from the DAG, you may stop the dag_cleaner using:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"DataFlowTasks.stop_dag_cleaner(taskgraph)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Finished nodes will now remain in the DAG:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"A = ones(5)\nDataFlowTasks.@spawn begin \n    @RW A\n    A .= 2 .* A\nend\nDataFlowTasks.@spawn sum(@R A)\ntaskgraph","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Note that stopping the dag_cleaner means finished nodes are no longer removed; since the task graph is a buffered structure, this may cause the execution to halt if it is at full capacity. You can then either resize! it, or simply start the worker (which will result in the processing of the finished channel):","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"DataFlowTasks.start_dag_cleaner(taskgraph)\ntaskgraph","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"tip: Tip\nThere are situations where you may want to use a different TaskGraph temporarily to execute a block of code, and restore the default after. This can be done using the with_taskgraph method.","category":"page"},{"location":"#Limitations","page":"Getting started","title":"Limitations","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"Some current limitations are listed below:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"There is no way to specify priorities for a task.\nThe main thread executes tasks, and is responsible for adding/removing nodes from the DAG. This may hinder parallelism if the main thread is given a long task since the processing of the dag will halt until the main thread becomes free again.\n...","category":"page"},{"location":"profiling/#visualization-section","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"DataFlowTasks defines two visualization tools that help when debugging and profiling parallel programs:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"a visualization of the Directed Acyclic Graph (DAG) internally representing task dependencies;\na visualization of how tasks were scheduled during a run, alongside with other information helping understand what limits the performances of the computation.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"note: Note\nVisualization tools require additional dependencies (such as Makie or GraphViz) which are only needed during the development stage. We are therefore only declaring those as weak dependencies (for Julia v1.9 and above). The user can either set up a stacked environment in which these dependencies are available, or use the DataFlowTasks.stack_weakdeps_env!() function which handles the environment stack automatically.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Let's first introduce a small example that will help illustrate the features introduced here:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"using DataFlowTasks\nusing DataFlowTasks: @spawn\nDataFlowTasks.stack_weakdeps_env!() #hide\n\n# Utility functions\ninit!(x)    = (x .= rand())     # Write\nmutate!(x)  = (x .= exp.(x))    # Read+Write\nresult(x,y) = sum(x) + sum(y)   # Read\n\n# Main work function\nfunction work(A, B)\n    # Initialization\n    @spawn init!(@W(A))               label=\"init A\"\n    @spawn init!(@W(B))               label=\"init B\"\n\n    # Mutation\n    @spawn mutate!(@RW(A))            label=\"mutate A\"\n    @spawn mutate!(@RW(B))            label=\"mutate B\"\n\n    # Final read\n    res = @spawn result(@R(A), @R(B)) label=\"read A,B\"\n    fetch(res)\nend","category":"page"},{"location":"profiling/#Creating-a-[LogInfo](@ref-DataFlowTasks.LogInfo)","page":"Debugging & Profiling","title":"Creating a LogInfo","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"In order to inspect code which makes use of DataFlowTasks, you can use the DataFlowTasks.@log macro to keep a trace of the various parallel events and the underlying DAG. Note that to avoid profiling the compilation time, it is often advisable to perform a \"dry run\" of the code first, as done in the example below:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"\n# Context\nA = ones(2000, 2000)\nB = ones(3000, 3000)\n\n# precompilation run\nwork(copy(A),copy(B)) \n\n# activate logging of events\nlog_info = DataFlowTasks.@log work(A, B)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The log_info object above, of LogInfo type, stores various information that can be used to reconstruct both the inferred dependencies and the parallel execution traces of the DataFlowTasks, as illustrated next.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"warning: Warning\nWhen using @log, you typically want the block of code being benchmarked to wait for the completion of its DataFlowTasks before returning (otherwise the LogInfo object that is returned may lack information regarding the DataFlowTasks that have not been completed). In the example above, that was achieved through the use of fetch in the last line of the work function.","category":"page"},{"location":"profiling/#DAG-visualization","page":"Debugging & Profiling","title":"DAG visualization","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"In order to better understand what this example does, and check that data dependencies were suitably annotated, it can be useful to look at the Directed Acyclic Graph (DAG) representing task dependencies as they were inferred by DataFlowTasks. The DAG can be visualized by creating a GraphViz.Graph out of it:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"using GraphViz\nGraphViz.Graph(log_info)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When the working environment supports rich media, the DAG will be displayed automatically. In other cases, it is possible to export it to an image using DataFlowTasks.savedag:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"dag = GraphViz.Graph(log_info)\nDataFlowTasks.savedag(\"profiling-example.svg\", dag)\nnothing # hide","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Note how the task labels (which were provided as extra arguments to @spawn) are used in the DAG rendering and make it more readable. In the DAG visualization, the critical path is highlighted in red: it is the sequential path that took the longest run time during the computation.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"note: Note\nThe run time of this critical path imposes a hard bound on parallel performances: no matter how many threads are available, it is not possible for the computation to take less time than the duration of the critical path.","category":"page"},{"location":"profiling/#Scheduling-and-profiling-information","page":"Debugging & Profiling","title":"Scheduling and profiling information","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The collected scheduling & profiling information can be visualized using Makie.plot on the log_info object (note that using the GLMakie backend brings a bit more interactivity than CairoMakie):","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"using CairoMakie # or GLMakie to benefit from more interactivity\nplot(log_info; categories=[\"init\", \"mutate\", \"read\"])\nnothing # hide","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"(Image: ProfilingExampleTrace)","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The categories keyword argument allows grouping tasks in categories according to their labels. In the example above, all tasks containing \"mutate\" in their label will be grouped in the same category.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Note : be careful with giving similar labels. If tasks have \"R\" and \"RW\" labels, and the substrings given to the plot's argument are also \"R\", and \"RW\", then all tasks will be in the category \"R\" (because \"R\" can be found in \"RW\"). Regular expressions can be given instead of substrings in order to avoid such issues.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Let us explore the various parts of this graph.","category":"page"},{"location":"profiling/#Parallel-Trace","page":"Debugging & Profiling","title":"Parallel Trace","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The main plot (at the top) is the parallel trace visualization. In this example there were two threads; we can see on which thread the task was run, and the time it took.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Even though tasks are grouped in categories by considering substrings in their labels, the full label is shown when hovering over a task in the interactive visualization (i.e. when using GLMakie instead of CairoMakie).","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The plot also shows the time spent inserting nodes in the graph (which is part of the overhead incurred by the use of DataFlowTasks): these insertion times are visualized as red tasks. They are not visible for such a small example, but the interactive visualization allows zooming on the plot to search for those small tasks.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Also note that inserting tasks into the graph involves memory allocations, and may thus trigger garbage collector sweeps. When this happens, the time spent in the garbage collector is also shown in the plot.","category":"page"},{"location":"profiling/#Activity-plot","page":"Debugging & Profiling","title":"Activity plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"activity\" barplot (in the bottom left corner of the window) gives us information on the break-down of parallel computing times (summed over all threads):","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"Computing represents the total time spent in the tasks bodies (i.e. \"useful\" work);\nInserting represents the total time spent inserting nodes in the DAG (i.e. overhead induced by DataFlowTasks), possibly including any time spent in the GC if it is triggered by a memory allocation in the task insertion process;\nOther represents the total idle time on all threads (which may be due to bad scheduling, or simply arise by lack of enough exposed parallelism in the algorithm).","category":"page"},{"location":"profiling/#Time-Bounds-plot","page":"Debugging & Profiling","title":"Time Bounds plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"Time Bounds\" barplot (in the bottom center of the window) tries to present insightful information about the performance limiting factors in the computation:","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"critical path represents the time spent in the longest sequential path in the DAG (shown in red in the DAG visualization). As said above, it bounds the performance in that even infinitely many threads would still have to compute this path sequentially;\nwithout waiting represents the duration of a hypothetical computation in which all computing time would be evenly distributed among threads (i.e. no thread would ever have to wait). This also bounds the total time because it does not account for dependencies between tasks.\nReal represents the measured \"wall clock time\" of the computation; it should be larger than both of the aforementioned bounds.","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When looking for faster response times, this graph may suggest sensible ways to explore. If the measured time is close to the critical path duration, then adding more threads will be of no help, but decomposing the work in smaller tasks may be useful. On the other hand, if the measured time is close to the \"without waiting\" bound, then adding more workers may reduce the wall clock time and scale relatively well.","category":"page"},{"location":"profiling/#Times-per-Category-plot","page":"Debugging & Profiling","title":"Times per Category plot","text":"","category":"section"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"The \"Times per Category\" barplot (in the bottom right of the window) displays the total time spent on all threads while performing user-defined tasks (grouped by category as explained above).","category":"page"},{"location":"profiling/","page":"Debugging & Profiling","title":"Debugging & Profiling","text":"When trying to optimize the sequential performance of the algorithm, this is where one can get data about what actually takes time (and therefore could produce large gains in performance if it could be optimized).","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"EditURL = \"cholesky.jl\"","category":"page"},{"location":"examples/cholesky/cholesky/#tiledcholesky-section","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: ipynb) (Image: nbviewer)","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"We illustrate here the use of DataFlowTasks to parallelize a tiled Cholesky factorization. The implementation shown here is delibarately made as simple and self-contained as possible; a more complex and more efficient implementation can be found in the TiledFactorization package.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The Cholesky factorization algorithm takes a symmetric positive definite matrix A and finds a lower triangular matrix L such that A = LLᵀ. The tiled version of this algorithm decomposes the matrix A into tiles (of even sizes, in this simplified version). At each step of the algorithm, we do a Cholesky factorization on the diagonal tile, use a triangular solve to update all of the tiles at the right of the diagonal tile, and finally update all the tiles of the submatrix with a schur complement.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"If we have a matrix A decomposed in n times n tiles, then the algorithm will have n steps. The i-th step (with i in 1n) performs:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":" 1 cholesky factorization of the (ii) tile,\n (i-1) triangular solves (one for each tile in the i-th row of the upper triangular matrix),\n i(i-1)2 matrix multiplications to update the submatrix.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"These are the basic operations on tiles, which we are going to spawn in separate tasks in the parallel implementation. Accounting for all iterations, this makes a total of mathcalO(n^3) such tasks, decomposed as:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":" mathcalO(n) cholesky factorizations,\n mathcalO(n^2) triangular solves,\n mathcalO(n^3) matrix multiplications.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The following image illustrates the 2nd step of the algorithm:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/#Sequential-implementation","page":"Tiled Cholesky Factorization","title":"Sequential implementation","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"A sequential tiled factorization algorithm can be implemented as:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using LinearAlgebra\n\ntilerange(ti, ts) = (ti-1)*ts+1:ti*ts\n\nfunction cholesky_tiled!(A, ts)\n    m = size(A, 1); @assert m==size(A, 2)\n    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n    n = m÷ts  # number of tiles in each dimension\n\n    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n\n    for i in 1:n\n        # Diagonal cholesky serial factorization\n        cholesky!(T[i,i])\n\n        # Left tiles update\n        U = UpperTriangular(T[i,i])\n        for j in i+1:n\n            ldiv!(U', T[i,j])\n        end\n\n        # Submatrix update\n        for j in i+1:n\n            for k in j:n\n                mul!(T[j,k], T[i,j]', T[i,k], -1, 1)\n            end\n        end\n    end\n\n    # Construct the factorized object\n    return Cholesky(A, 'U', zero(LinearAlgebra.BlasInt))\nend","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Let us build a small test case to check the correctness of the factorization. Here we divide a matrix of size 4096×4096 in 8×8 tiles of size 512×512:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"n  = 4096\nts = 512\nA = rand(n, n)\nA = (A + adjoint(A))/2\nA = A + n*I;\nnothing #hide","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"and the results seem to be correct:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"F = cholesky_tiled!(copy(A), ts)\n\n# Check results\nerr = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n@show err\n@assert err < eps(Float64)","category":"page"},{"location":"examples/cholesky/cholesky/#Parallel-implementation","page":"Tiled Cholesky Factorization","title":"Parallel implementation","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"In order to parallelize the code with DataFlowTasks.jl, function calls acting on tiles are wrapped within @spawn, along with annotations describing data access modes. We also give meaningful labels to the tasks, which will help debug and profile the code.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using DataFlowTasks\nusing DataFlowTasks: @spawn\n\nfunction cholesky_dft!(A, ts)\n    m = size(A, 1); @assert m==size(A, 2)\n    m%ts != 0 && error(\"Tilesize doesn't fit the matrix\")\n    n = m÷ts  # number of tiles in each dimension\n\n    T = [view(A, tilerange(i, ts), tilerange(j, ts)) for i in 1:n, j in 1:n]\n\n    for i in 1:n\n        # Diagonal cholesky serial factorization\n        @spawn cholesky!(@RW(T[i,i])) label=\"chol ($i,$i)\"\n\n        # Left tiles update\n        U = UpperTriangular(T[i,i])\n        for j in i+1:n\n            @spawn ldiv!(@R(U)', @RW(T[i,j])) label=\"ldiv ($i,$j)\"\n        end\n\n        # Submatrix update\n        for j in i+1:n\n            for k in j:n\n                @spawn mul!(@RW(T[j,k]), @R(T[i,j])', @R(T[i,k]), -1, 1) label=\"schur ($j,$k)\"\n            end\n        end\n    end\n\n    # Construct the factorized object\n    r = @spawn Cholesky(@R(A), 'U', zero(LinearAlgebra.BlasInt)) label=\"result\"\n    return fetch(r)\nend","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Again, let us check the correctness of the result:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"F = cholesky_dft!(copy(A), ts)\n\n# Check results\nerr = norm(F.L*F.U-A,Inf)/max(norm(A),norm(F.L*F.U))\n@show err\n@assert err < eps(Float64)","category":"page"},{"location":"examples/cholesky/cholesky/#Debugging-and-Profiling","page":"Tiled Cholesky Factorization","title":"Debugging and Profiling","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"Let us now check what happens during a parallel run of our cholesky factorization. Thanks to the test above, the code is now compiled. Let's re-run it and collect meaningful profiling information:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"# Clean profiling environment\nGC.gc()\n\n# Real workload to be analysed\nAc = copy(A)\nlog_info = DataFlowTasks.@log cholesky_dft!(Ac, ts)","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The number of tasks being mathcalO(n^3), we can see how quickly the DAG complexity increases (even though the test case only has 8×8 tiles here):","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"DataFlowTasks.stack_weakdeps_env!()\nusing GraphViz\ndag = GraphViz.Graph(log_info)","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The critical path, highlighted in red, includes all cholesky factorizations of diagonal tiles, as well as the required tasks in between them.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The parallel trace plot gives us more details about the performance limiting factors:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"using CairoMakie # or GLMakie in order to have more interactivity\ntrace = plot(log_info; categories=[\"chol\", \"ldiv\", \"schur\"])","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The overhead incurred by DataFlowTasks seems relatively small here: the time taken inserting tasks is barely measurable, and the scheduling did not lead to threads waiting idly for too long. This is confirmed by the \"Time Bounds\" plot, showing a measured wall clock time not too much longer than the lower bound obtained when suppressing idle time.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The \"Times per Category\" plot seems to indicate that the matrix multiplications performed in the \"Schur\" tasks account for the majority of the computing time. Trying to optimize these would be a priority to increase the sequential performance of the factorization.","category":"page"},{"location":"examples/cholesky/cholesky/#Performances","page":"Tiled Cholesky Factorization","title":"Performances","text":"","category":"section"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The performance of this example can be improved by using better implementations for the sequential building blocks operating on tiles:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"LoopVectorization.jl can improve the performance of the sequential cholesky factorization of diagonal blocks as well as the schur_complement\nTriangularSolve.jl provides a high-performance ldiv! implementation","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"This approach is pursued in TiledFactorization.jl, where all the above mentioned building blocks are combined with the parallelization strategy presented here to create a pure Julia implementation of the matrix factorizations. The performances of this implementation is assessed in the following plot, by comparison to MKL on a the case of a 5000x5000 matrix decomposed in tiles of size 256x256.","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"The figure above was generated by running this script on a machine with 2x10 Intel Xeon Silver 4114 cores (2.20GHz) with the following topology:","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"(Image: )","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"","category":"page"},{"location":"examples/cholesky/cholesky/","page":"Tiled Cholesky Factorization","title":"Tiled Cholesky Factorization","text":"This page was generated using Literate.jl.","category":"page"}]
}
